{"posts":[{"title":"Javascript 5 - DOM","content":" position: absolute; An element with position: absolute; is positioned relative to the nearest positioned ancestor position: relative; An element with position: relative; is positioned relative to its normal position. &lt;div id=&quot;box1&quot;&gt;&lt;!--参照定位的元素--&gt; &lt;div id=&quot;box2&quot;&gt;相对参照元素进行定位&lt;/div&gt;&lt;!--相对定位元素--&gt; &lt;/div&gt; #box1{ width:200px; height:200px; position:relative; } #box2{ position:absolute; top:20px; left:30px; } **已知宽高实现盒子水平垂直居中 ** &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;已知宽高实现盒子水平垂直居中&lt;/title&gt; &lt;style type=&quot;text/css&quot;&gt; .box { border: 1px solid #00ee00; height: 300px; position: relative; } .box1 { position: absolute; top: 50%; left: 50%; margin: -100px 0px 0px -100px; width: 200px; height: 200px; border: 1px solid red; } &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;box&quot;&gt; &lt;div class=&quot;box1&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;/body&gt; 宽高不定实现盒子水平垂直居中 &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;宽高不定实现盒子水平垂直居中&lt;/title&gt; &lt;style type=&quot;text/css&quot;&gt; .box { border: 1px solid #00ee00; height: 300px; position: relative; } .box1 { border: 1px solid red; position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); } &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;box&quot;&gt; &lt;div class=&quot;box1&quot;&gt; 网网网网网网网网网网网网网网网网网网网网网网 &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; ","link":"https://blog.ferretninja.com/post/javascript-5-dom/"},{"title":"Javascript 4 - ES6","content":" let/const block scope let arr = [1,2,3,4] for (let i = 0; i &lt; arr.length; i++) { } console.log(i); // error let arr = [1,2,3,4] for (var i = 0; i &lt; arr.length; i++) { } console.log(i); // 4 Hoisting console.log(foo); var foo = 1; Arrow function Arrow function no this, cannot be constructor, no prototype // 函数声明 function test() {} // 函数表达式 const test = function() {} // 箭头函数 const test = () =&gt; {} var obj = { commonFn : function() { console.log(this); }, arrowFn : () =&gt; { console.log(this); } } obj.commonFn(); obj.arrowFn(); VM372:3 {commonFn: ƒ, arrowFn: ƒ} VM372:6 Window {parent: Window, opener: null, top: Window, length: 2, frames: Window, …} Template string let getName = () =&gt; { return 'test'; } let str = ` &lt;div&gt; &lt;h1 class = &quot;title&quot;&gt;${getName()}&lt;/h1&gt; &lt;/div&gt; `; document.querySelector('body').innerHTML = str; object var name = 'test', age = 18; var obj = { name: name, age: age, getName: function() { return this.name; }, getAge: function() { return this.age; } } let name = 'test', age = 18; let obj = { name, age, getName() { return this.name; }, ['get' + 'Age']() { return this.age; } } ","link":"https://blog.ferretninja.com/post/javascript-4-es6/"},{"title":"Javascript 3 - http request","content":"url parsing dns request dom ","link":"https://blog.ferretninja.com/post/javascript-3-http-request/"},{"title":"Javascript 2 - Chrome dev tools","content":"ctrl + shift + i open dev tools ctrl + shift + C select element network: request status ","link":"https://blog.ferretninja.com/post/javascript-2-chrome-dev-tools/"},{"title":"AWS Redshift and Apache Airflow pipeline","content":"A reusable production-grade data pipeline that incorporates data quality checks and allows for easy backfills. The source data resides in S3 and needs to be processed in a data warehouse in Amazon Redshift. The source datasets consist of JSON logs that tell about user activity in the application and JSON metadata about the songs the users listen to. Create AWS redshift cluster and test queries. Set up AWS S3 hook Set up redshift connection hook Set up Airflow job DAG Run Airflow scheduler See past job statistics ","link":"https://blog.ferretninja.com/post/aws-redshift-and-apache-airflow-pipeline/"},{"title":"AWS EMR Spark and Data Lake","content":" An ETL pipeline that extracts data from S3, processes them using Spark, and loads the data back into S3 as a set of dimensional tables. Create a Data Lake with Spark and AWS EMR create a ssh key-pair to securely connect to the EMR cluster create an EMR cluster ssh into the master node access master node jupyter notebook print(&quot;Welcome to my EMR Notebook!&quot;) VBox() Starting Spark application IDYARN Application IDKindStateSpark UIDriver logCurrent session?1application_1573680517609_0004pysparkidleLinkLink✔ FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),… SparkSession available as 'spark'. FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),… Welcome to my EMR Notebook! %%info Current session configs: {'conf': {'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'kind': 'pyspark'} IDYARN Application IDKindStateSpark UIDriver logCurrent session?1application_1573680517609_0004pysparkidleLinkLink✔ sc.list_packages() VBox() FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),… Package Version -------------------------- ------- beautifulsoup4 4.8.1 boto 2.49.0 jmespath 0.9.4 lxml 4.4.1 mysqlclient 1.4.4 nltk 3.4.5 nose 1.3.4 numpy 1.14.5 pip 19.3.1 py-dateutil 2.2 python36-sagemaker-pyspark 1.2.6 pytz 2019.3 PyYAML 3.11 setuptools 41.6.0 six 1.12.0 soupsieve 1.9.4 wheel 0.33.6 windmill 1.6 sc.install_pypi_package(&quot;pandas==0.25.1&quot;) VBox() FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),… Collecting pandas==0.25.1 Downloading https://files.pythonhosted.org/packages/73/9b/52e228545d14f14bb2a1622e225f38463c8726645165e1cb7dde95bfe6d4/pandas-0.25.1-cp36-cp36m-manylinux1_x86_64.whl (10.5MB) Requirement already satisfied: numpy&gt;=1.13.3 in /usr/local/lib64/python3.6/site-packages (from pandas==0.25.1) (1.14.5) Collecting python-dateutil&gt;=2.6.1 Downloading https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas==0.25.1) (2019.3) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil&gt;=2.6.1-&gt;pandas==0.25.1) (1.12.0) Installing collected packages: python-dateutil, pandas Successfully installed pandas-0.25.1 python-dateutil-2.8.1 sc.install_pypi_package(&quot;matplotlib&quot;, &quot;https://pypi.org/simple&quot;) #Install matplotlib from given PyPI repository VBox() FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),… Collecting matplotlib Downloading https://files.pythonhosted.org/packages/57/4f/dd381ecf6c6ab9bcdaa8ea912e866dedc6e696756156d8ecc087e20817e2/matplotlib-3.1.1-cp36-cp36m-manylinux1_x86_64.whl (13.1MB) Requirement already satisfied: numpy&gt;=1.11 in /usr/local/lib64/python3.6/site-packages (from matplotlib) (1.14.5) Collecting kiwisolver&gt;=1.0.1 Downloading https://files.pythonhosted.org/packages/f8/a1/5742b56282449b1c0968197f63eae486eca2c35dcd334bab75ad524e0de1/kiwisolver-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (90kB) Requirement already satisfied: python-dateutil&gt;=2.1 in /mnt/tmp/1573683191129-0/lib/python3.6/site-packages (from matplotlib) (2.8.1) Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 Downloading https://files.pythonhosted.org/packages/c0/0c/fc2e007d9a992d997f04a80125b0f183da7fb554f1de701bbb70a8e7d479/pyparsing-2.4.5-py2.py3-none-any.whl (67kB) Collecting cycler&gt;=0.10 Downloading https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl Requirement already satisfied: setuptools in /mnt/tmp/1573683191129-0/lib/python3.6/site-packages (from kiwisolver&gt;=1.0.1-&gt;matplotlib) (41.6.0) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil&gt;=2.1-&gt;matplotlib) (1.12.0) Installing collected packages: kiwisolver, pyparsing, cycler, matplotlib Successfully installed cycler-0.10.0 kiwisolver-1.1.0 matplotlib-3.1.1 pyparsing-2.4.5 sc.list_packages() VBox() FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),… Package Version -------------------------- ------- beautifulsoup4 4.8.1 boto 2.49.0 cycler 0.10.0 jmespath 0.9.4 kiwisolver 1.1.0 lxml 4.4.1 matplotlib 3.1.1 mysqlclient 1.4.4 nltk 3.4.5 nose 1.3.4 numpy 1.14.5 pandas 0.25.1 pip 19.3.1 py-dateutil 2.2 pyparsing 2.4.5 python-dateutil 2.8.1 python36-sagemaker-pyspark 1.2.6 pytz 2019.3 PyYAML 3.11 setuptools 41.6.0 six 1.12.0 soupsieve 1.9.4 wheel 0.33.6 windmill 1.6 df = spark.read.parquet('s3://amazon-reviews-pds/parquet/product_category=Books/*.parquet') VBox() FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),… df.printSchema() num_of_books = df.select('product_id').distinct().count() print(f'Number of Books: {num_of_books:,}') VBox() FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),… root |-- marketplace: string (nullable = true) |-- customer_id: string (nullable = true) |-- review_id: string (nullable = true) |-- product_id: string (nullable = true) |-- product_parent: string (nullable = true) |-- product_title: string (nullable = true) |-- star_rating: integer (nullable = true) |-- helpful_votes: integer (nullable = true) |-- total_votes: integer (nullable = true) |-- vine: string (nullable = true) |-- verified_purchase: string (nullable = true) |-- review_headline: string (nullable = true) |-- review_body: string (nullable = true) |-- review_date: date (nullable = true) |-- year: integer (nullable = true) Number of Books: 3,423,743 install python libraries sudo easy_install-3.6 pip sudo /usr/local/bin/pip3 install paramiko nltk scipy scikit-learn pandas upload file to EMR spark-submit job import configparser from datetime import datetime import os from pyspark.sql import SparkSession from pyspark.sql.functions import udf, col from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format from pyspark.sql import functions as F from pyspark.sql import types as T import pandas as pd pd.set_option('display.max_columns', 500) config = configparser.ConfigParser() #Normally this file should be in ~/.aws/credentials config.read_file(open('dl.cfg')) os.environ[&quot;AWS_ACCESS_KEY_ID&quot;]= config['AWS']['AWS_ACCESS_KEY_ID'] os.environ[&quot;AWS_SECRET_ACCESS_KEY&quot;]= config['AWS']['AWS_SECRET_ACCESS_KEY'] def create_spark_session(): spark = SparkSession \\ .builder \\ .config(&quot;spark.jars.packages&quot;, &quot;org.apache.hadoop:hadoop-aws:2.7.0&quot;) \\ .getOrCreate() return spark # create the spark session spark = create_spark_session() # read data from my S3 bucket. This is the same data in workspace songPath = 's3a://testemrs3/song_data/*/*/*/*.json' logPath = 's3a://testemrs3/log_data/*.json' # define output paths output = 's3a://testemrs3/schema/' process song data create song_table # Step 1: Read in the song data df_song = spark.read.json(songPath) # check the schema df_song.printSchema() root |-- artist_id: string (nullable = true) |-- artist_latitude: double (nullable = true) |-- artist_location: string (nullable = true) |-- artist_longitude: double (nullable = true) |-- artist_name: string (nullable = true) |-- duration: double (nullable = true) |-- num_songs: long (nullable = true) |-- song_id: string (nullable = true) |-- title: string (nullable = true) |-- year: long (nullable = true) # Step 2: extract columns to create songs table song_cols = ['song_id', 'title', 'artist_id', 'year', 'duration'] # groupby song_id and select the first record's title in the group. t1 = df_song.select(F.col('song_id'), 'title') \\ .groupBy('song_id') \\ .agg({'title': 'first'}) \\ .withColumnRenamed('first(title)', 'title1') t2 = df_song.select(song_cols) t1.toPandas().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } song_id title1 0 SOGOSOV12AF72A285E ¿Dónde va Chichi? 1 SOMZWCG12A8C13C480 I Didn't Mean To 2 SOUPIRU12A6D4FA1E1 Der Kleine Dompfaff 3 SOXVLOJ12AB0189215 Amor De Cabaret 4 SOWTBJW12AC468AC6E Broken-Down Merry-Go-Round t2.toPandas().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } song_id title artist_id year duration 0 SOBAYLL12A8C138AF9 Sono andati? Fingevo di dormire ARDR4AC1187FB371A1 0 511.16363 1 SOOLYAZ12A6701F4A6 Laws Patrolling (Album Version) AREBBGV1187FB523D2 0 173.66159 2 SOBBUGU12A8C13E95D Setting Fire to Sleeping Giants ARMAC4T1187FB3FA4C 2004 207.77751 3 SOAOIBZ12AB01815BE I Hold Your Hand In Mine [Live At Royal Albert... ARPBNLO1187FB3D52F 2000 43.36281 4 SONYPOM12A8C13B2D7 I Think My Wife Is Running Around On Me (Taco ... ARDNS031187B9924F0 2005 186.48771 song_table_df = t1.join(t2, 'song_id') \\ .where(F.col(&quot;title1&quot;) == F.col(&quot;title&quot;)) \\ .select(song_cols) song_table_df.toPandas().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } song_id title artist_id year duration 0 SOBAYLL12A8C138AF9 Sono andati? Fingevo di dormire ARDR4AC1187FB371A1 0 511.16363 1 SOOLYAZ12A6701F4A6 Laws Patrolling (Album Version) AREBBGV1187FB523D2 0 173.66159 2 SOBBUGU12A8C13E95D Setting Fire to Sleeping Giants ARMAC4T1187FB3FA4C 2004 207.77751 3 SOAOIBZ12AB01815BE I Hold Your Hand In Mine [Live At Royal Albert... ARPBNLO1187FB3D52F 2000 43.36281 4 SONYPOM12A8C13B2D7 I Think My Wife Is Running Around On Me (Taco ... ARDNS031187B9924F0 2005 186.48771 song_table_df.toPandas().shape (71, 5) df_song.toPandas().shape (71, 10) # Step 3: Write this to a parquet file song_table_df.write.parquet('data/songs_table', partitionBy=['year', 'artist_id'], mode='Overwrite') # write this to s3 bucket song_table_df.write.parquet(output + 'songs_table', partitionBy=['year', 'artist_id'], mode='Overwrite') create artists_table # define the cols artists_cols = [&quot;artist_id&quot;, &quot;artist_name&quot;, &quot;artist_location&quot;, &quot;artist_latitude&quot;, &quot;artist_longitude&quot;] df_song.printSchema() root |-- artist_id: string (nullable = true) |-- artist_latitude: double (nullable = true) |-- artist_location: string (nullable = true) |-- artist_longitude: double (nullable = true) |-- artist_name: string (nullable = true) |-- duration: double (nullable = true) |-- num_songs: long (nullable = true) |-- song_id: string (nullable = true) |-- title: string (nullable = true) |-- year: long (nullable = true) # groupby song_id and select the first record's title in the group. t1 = df_song.select(F.col('artist_id'), 'artist_name') \\ .groupBy('artist_id') \\ .agg({'artist_name': 'first'}) \\ .withColumnRenamed('first(artist_name)', 'artist_name1') t2 = df_song.select(artists_cols) t1.toPandas().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } artist_id artist_name1 0 AR9AWNF1187B9AB0B4 Kenny G featuring Daryl Hall 1 AR0IAWL1187B9A96D0 Danilo Perez 2 AR0RCMP1187FB3F427 Billie Jo Spears 3 AREDL271187FB40F44 Soul Mekanik 4 ARI3BMM1187FB4255E Alice Stuart t2.toPandas().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } artist_id artist_name artist_location artist_latitude artist_longitude 0 ARDR4AC1187FB371A1 Montserrat Caballé;Placido Domingo;Vicente Sar... NaN NaN 1 AREBBGV1187FB523D2 Mike Jones (Featuring CJ_ Mello &amp; Lil' Bran) Houston, TX NaN NaN 2 ARMAC4T1187FB3FA4C The Dillinger Escape Plan Morris Plains, NJ 40.82624 -74.47995 3 ARPBNLO1187FB3D52F Tiny Tim New York, NY 40.71455 -74.00712 4 ARDNS031187B9924F0 Tim Wilson Georgia 32.67828 -83.22295 artists_table_df = t1.join(t2, 'artist_id') \\ .where(F.col(&quot;artist_name1&quot;) == F.col(&quot;artist_name&quot;)) \\ .select(artists_cols) artists_table_df.toPandas().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } artist_id artist_name artist_location artist_latitude artist_longitude 0 ARDR4AC1187FB371A1 Montserrat Caballé;Placido Domingo;Vicente Sar... NaN NaN 1 AREBBGV1187FB523D2 Mike Jones (Featuring CJ_ Mello &amp; Lil' Bran) Houston, TX NaN NaN 2 ARMAC4T1187FB3FA4C The Dillinger Escape Plan Morris Plains, NJ 40.82624 -74.47995 3 ARPBNLO1187FB3D52F Tiny Tim New York, NY 40.71455 -74.00712 4 ARDNS031187B9924F0 Tim Wilson Georgia 32.67828 -83.22295 # write this to s3 bucket artists_table_df.write.parquet(output + 'artists_table', mode='Overwrite') artists_table_df.write.parquet('data/artists_table', mode='Overwrite') # read the partitioned data df_artists_read = spark.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).parquet(&quot;data/artists_table&quot;) Process log data # Step 1: Read in the log data df_log = spark.read.json(logPath) df_log.printSchema() root |-- artist: string (nullable = true) |-- auth: string (nullable = true) |-- firstName: string (nullable = true) |-- gender: string (nullable = true) |-- itemInSession: long (nullable = true) |-- lastName: string (nullable = true) |-- length: double (nullable = true) |-- level: string (nullable = true) |-- location: string (nullable = true) |-- method: string (nullable = true) |-- page: string (nullable = true) |-- registration: double (nullable = true) |-- sessionId: long (nullable = true) |-- song: string (nullable = true) |-- status: long (nullable = true) |-- ts: long (nullable = true) |-- userAgent: string (nullable = true) |-- userId: string (nullable = true) df_log.filter(F.col(&quot;page&quot;) == &quot;NextSong&quot;).toPandas().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } artist auth firstName gender itemInSession lastName length level location method page registration sessionId song status ts userAgent userId 0 Harmonia Logged In Ryan M 0 Smith 655.77751 free San Jose-Sunnyvale-Santa Clara, CA PUT NextSong 1.541017e+12 583 Sehr kosmisch 200 1542241826796 \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5... 26 1 The Prodigy Logged In Ryan M 1 Smith 260.07465 free San Jose-Sunnyvale-Santa Clara, CA PUT NextSong 1.541017e+12 583 The Big Gundown 200 1542242481796 \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5... 26 2 Train Logged In Ryan M 2 Smith 205.45261 free San Jose-Sunnyvale-Santa Clara, CA PUT NextSong 1.541017e+12 583 Marry Me 200 1542242741796 \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5... 26 3 Sony Wonder Logged In Samuel M 0 Gonzalez 218.06975 free Houston-The Woodlands-Sugar Land, TX PUT NextSong 1.540493e+12 597 Blackbird 200 1542253449796 \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4... 61 4 Van Halen Logged In Tegan F 2 Levine 289.38404 paid Portland-South Portland, ME PUT NextSong 1.540794e+12 602 Best Of Both Worlds (Remastered Album Version) 200 1542260935796 \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4... 80 # Step 2: filter by actions for song plays df_log = df_log.filter(F.col(&quot;page&quot;) == &quot;NextSong&quot;) df_log.toPandas().shape (6820, 18) # Step 3: extract columns for users table users_cols = [&quot;userId&quot;, &quot;firstName&quot;, &quot;lastName&quot;, &quot;gender&quot;, &quot;level&quot;] df_log.select(users_cols).toPandas().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } userId firstName lastName gender level 0 26 Ryan Smith M free 1 26 Ryan Smith M free 2 26 Ryan Smith M free 3 61 Samuel Gonzalez M free 4 80 Tegan Levine F paid df_log.select(users_cols).toPandas().shape (6820, 5) df_log.select(users_cols).dropDuplicates().toPandas().shape (104, 5) df_log.select(users_cols).dropDuplicates().toPandas().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } userId firstName lastName gender level 0 57 Katherine Gay F free 1 84 Shakira Hunt F free 2 22 Sean Wilson F free 3 52 Theodore Smith M free 4 80 Tegan Levine F paid users_table_df = df_log.select(users_cols).dropDuplicates() # write this to s3 bucket users_table_df.write.parquet(output + 'users_table', mode='Overwrite') users_table_df.write.parquet('data/users_table', mode='Overwrite') Time table # # create timestamp column from original timestamp column get_timestamp = udf() df_log.select('ts').toPandas().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ts 0 1542241826796 1 1542242481796 2 1542242741796 3 1542253449796 4 1542260935796 df.withColumn('epoch', f.date_format(df.epoch.cast(dataType=t.TimestampType()), &quot;yyyy-MM-dd&quot;)) df_log.withColumn('ts', F.date_format(df_log.ts.cast(dataType=T.TimestampType()), &quot;yyyy-MM-dd&quot;)).toPandas().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } artist auth firstName gender itemInSession lastName length level location method page registration sessionId song status ts userAgent userId 0 Harmonia Logged In Ryan M 0 Smith 655.77751 free San Jose-Sunnyvale-Santa Clara, CA PUT NextSong 1.541017e+12 583 Sehr kosmisch 200 50841-09-12 \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5... 26 1 The Prodigy Logged In Ryan M 1 Smith 260.07465 free San Jose-Sunnyvale-Santa Clara, CA PUT NextSong 1.541017e+12 583 The Big Gundown 200 50841-09-19 \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5... 26 2 Train Logged In Ryan M 2 Smith 205.45261 free San Jose-Sunnyvale-Santa Clara, CA PUT NextSong 1.541017e+12 583 Marry Me 200 50841-09-22 \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5... 26 3 Sony Wonder Logged In Samuel M 0 Gonzalez 218.06975 free Houston-The Woodlands-Sugar Land, TX PUT NextSong 1.540493e+12 597 Blackbird 200 50842-01-24 \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4... 61 4 Van Halen Logged In Tegan F 2 Levine 289.38404 paid Portland-South Portland, ME PUT NextSong 1.540794e+12 602 Best Of Both Worlds (Remastered Album Version) 200 50842-04-21 \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4... 80 df_time = df_log.select('ts') df_time.take(5) [Row(ts=1542241826796), Row(ts=1542242481796), Row(ts=1542242741796), Row(ts=1542253449796), Row(ts=1542260935796)] @udf def gettimestamp(time): import datetime time = time/1000 return datetime.datetime.fromtimestamp(time).strftime(&quot;%m-%d-%Y %H:%M:%S&quot;) df_time.withColumn(&quot;timestamp&quot;, gettimestamp(&quot;ts&quot;)).show() +-------------+-------------------+ | ts| timestamp| +-------------+-------------------+ |1542241826796|11-15-2018 00:30:26| |1542242481796|11-15-2018 00:41:21| |1542242741796|11-15-2018 00:45:41| |1542253449796|11-15-2018 03:44:09| |1542260935796|11-15-2018 05:48:55| |1542261224796|11-15-2018 05:53:44| |1542261356796|11-15-2018 05:55:56| |1542261662796|11-15-2018 06:01:02| |1542262057796|11-15-2018 06:07:37| |1542262233796|11-15-2018 06:10:33| |1542262434796|11-15-2018 06:13:54| |1542262456796|11-15-2018 06:14:16| |1542262679796|11-15-2018 06:17:59| |1542262728796|11-15-2018 06:18:48| |1542262893796|11-15-2018 06:21:33| |1542263158796|11-15-2018 06:25:58| |1542263378796|11-15-2018 06:29:38| |1542265716796|11-15-2018 07:08:36| |1542265929796|11-15-2018 07:12:09| |1542266927796|11-15-2018 07:28:47| +-------------+-------------------+ only showing top 20 rows df_time.printSchema() root |-- ts: long (nullable = true) get_timestamp = F.udf(lambda x: datetime.fromtimestamp( (x/1000.0) ), T.TimestampType()) get_hour = F.udf(lambda x: x.hour, T.IntegerType()) get_day = F.udf(lambda x: x.day, T.IntegerType()) get_week = F.udf(lambda x: x.isocalendar()[1], T.IntegerType()) get_month = F.udf(lambda x: x.month, T.IntegerType()) get_year = F.udf(lambda x: x.year, T.IntegerType()) get_weekday = F.udf(lambda x: x.weekday(), T.IntegerType()) df_log = df_log.withColumn(&quot;timestamp&quot;, get_timestamp(df_log.ts)) df_log = df_log.withColumn(&quot;hour&quot;, get_hour(df_log.timestamp)) df_log = df_log.withColumn(&quot;day&quot;, get_day(df_log.timestamp)) df_log = df_log.withColumn(&quot;week&quot;, get_week(df_log.timestamp)) df_log = df_log.withColumn(&quot;month&quot;, get_month(df_log.timestamp)) df_log = df_log.withColumn(&quot;year&quot;, get_year(df_log.timestamp)) df_log = df_log.withColumn(&quot;weekday&quot;, get_weekday(df_log.timestamp)) df_log.limit(5).toPandas() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } artist auth firstName gender itemInSession lastName length level location method ... ts userAgent userId timestamp hour day week month year weekday 0 Harmonia Logged In Ryan M 0 Smith 655.77751 free San Jose-Sunnyvale-Santa Clara, CA PUT ... 1542241826796 \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5... 26 2018-11-15 00:30:26.796 0 15 46 11 2018 3 1 The Prodigy Logged In Ryan M 1 Smith 260.07465 free San Jose-Sunnyvale-Santa Clara, CA PUT ... 1542242481796 \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5... 26 2018-11-15 00:41:21.796 0 15 46 11 2018 3 2 Train Logged In Ryan M 2 Smith 205.45261 free San Jose-Sunnyvale-Santa Clara, CA PUT ... 1542242741796 \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5... 26 2018-11-15 00:45:41.796 0 15 46 11 2018 3 3 Sony Wonder Logged In Samuel M 0 Gonzalez 218.06975 free Houston-The Woodlands-Sugar Land, TX PUT ... 1542253449796 \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4... 61 2018-11-15 03:44:09.796 3 15 46 11 2018 3 4 Van Halen Logged In Tegan F 2 Levine 289.38404 paid Portland-South Portland, ME PUT ... 1542260935796 \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4... 80 2018-11-15 05:48:55.796 5 15 46 11 2018 3 5 rows × 25 columns time_cols = [&quot;timestamp&quot;, &quot;hour&quot;, &quot;day&quot;, &quot;week&quot;, &quot;month&quot;, &quot;year&quot;, &quot;weekday&quot;] time_table_df = df_log.select(time_cols) # write to parquet file partition by time_table_df.write.parquet('data/time_table', partitionBy=['year', 'month'], mode='Overwrite') # write this to s3 bucket time_table_df.write.parquet(output + 'time_table', partitionBy=['year', 'month'], mode='Overwrite') SongPlay table df_log.printSchema() root |-- artist: string (nullable = true) |-- auth: string (nullable = true) |-- firstName: string (nullable = true) |-- gender: string (nullable = true) |-- itemInSession: long (nullable = true) |-- lastName: string (nullable = true) |-- length: double (nullable = true) |-- level: string (nullable = true) |-- location: string (nullable = true) |-- method: string (nullable = true) |-- page: string (nullable = true) |-- registration: double (nullable = true) |-- sessionId: long (nullable = true) |-- song: string (nullable = true) |-- status: long (nullable = true) |-- ts: long (nullable = true) |-- userAgent: string (nullable = true) |-- userId: string (nullable = true) |-- timestamp: timestamp (nullable = true) |-- hour: integer (nullable = true) |-- day: integer (nullable = true) |-- week: integer (nullable = true) |-- month: integer (nullable = true) |-- year: integer (nullable = true) |-- weekday: integer (nullable = true) songplay_cols_temp = [&quot;timestamp&quot;, &quot;userId&quot;, &quot;sessionId&quot;, &quot;location&quot;, &quot;userAgent&quot;, &quot;level&quot;] df_log.select(songplay_cols).toPandas().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } timestamp userId sessionId location userAgent level 0 2018-11-15 00:30:26.796 26 583 San Jose-Sunnyvale-Santa Clara, CA \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5... free 1 2018-11-15 00:41:21.796 26 583 San Jose-Sunnyvale-Santa Clara, CA \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5... free 2 2018-11-15 00:45:41.796 26 583 San Jose-Sunnyvale-Santa Clara, CA \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5... free 3 2018-11-15 03:44:09.796 61 597 Houston-The Woodlands-Sugar Land, TX \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4... free 4 2018-11-15 05:48:55.796 80 602 Portland-South Portland, ME \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4... paid # read the partitioned data df_artists_read = spark.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).parquet(&quot;data/artists_table&quot;) df_songs_read = spark.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).parquet(&quot;data/songs_table&quot;) df_artists_read.toPandas().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } artist_id artist_name artist_location artist_latitude artist_longitude 0 ARDR4AC1187FB371A1 Montserrat Caballé;Placido Domingo;Vicente Sar... NaN NaN 1 AREBBGV1187FB523D2 Mike Jones (Featuring CJ_ Mello &amp; Lil' Bran) Houston, TX NaN NaN 2 ARMAC4T1187FB3FA4C The Dillinger Escape Plan Morris Plains, NJ 40.82624 -74.47995 3 ARPBNLO1187FB3D52F Tiny Tim New York, NY 40.71455 -74.00712 4 ARDNS031187B9924F0 Tim Wilson Georgia 32.67828 -83.22295 df_songs_read.toPandas().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } song_id title duration year artist_id 0 SOAOIBZ12AB01815BE I Hold Your Hand In Mine [Live At Royal Albert... 43.36281 2000 ARPBNLO1187FB3D52F 1 SONYPOM12A8C13B2D7 I Think My Wife Is Running Around On Me (Taco ... 186.48771 2005 ARDNS031187B9924F0 2 SODREIN12A58A7F2E5 A Whiter Shade Of Pale (Live @ Fillmore West) 326.00771 0 ARLTWXK1187FB5A3F8 3 SOYMRWW12A6D4FAB14 The Moon And I (Ordinary Day Album Version) 267.70240 0 ARKFYS91187B98E58F 4 SOWQTQZ12A58A7B63E Streets On Fire (Explicit Album Version) 279.97995 0 ARPFHN61187FB575F6 # merge song and artists df_songs_read.join(df_artists_read, 'artist_id').toPandas().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } artist_id song_id title duration year artist_name artist_location artist_latitude artist_longitude 0 ARPBNLO1187FB3D52F SOAOIBZ12AB01815BE I Hold Your Hand In Mine [Live At Royal Albert... 43.36281 2000 Tiny Tim New York, NY 40.71455 -74.00712 1 ARDNS031187B9924F0 SONYPOM12A8C13B2D7 I Think My Wife Is Running Around On Me (Taco ... 186.48771 2005 Tim Wilson Georgia 32.67828 -83.22295 2 ARLTWXK1187FB5A3F8 SODREIN12A58A7F2E5 A Whiter Shade Of Pale (Live @ Fillmore West) 326.00771 0 King Curtis Fort Worth, TX 32.74863 -97.32925 3 ARKFYS91187B98E58F SOYMRWW12A6D4FAB14 The Moon And I (Ordinary Day Album Version) 267.70240 0 Jeff And Sheri Easter NaN NaN 4 ARPFHN61187FB575F6 SOWQTQZ12A58A7B63E Streets On Fire (Explicit Album Version) 279.97995 0 Lupe Fiasco Chicago, IL 41.88415 -87.63241 df_joined_songs_artists = df_songs_read.join(df_artists_read, 'artist_id').select(&quot;artist_id&quot;, &quot;song_id&quot;, &quot;title&quot;, &quot;artist_name&quot;) songplay_cols = [&quot;timestamp&quot;, &quot;userId&quot;, &quot;song_id&quot;, &quot;artist_id&quot;, &quot;sessionId&quot;, &quot;location&quot;, &quot;userAgent&quot;, &quot;level&quot;, &quot;month&quot;, &quot;year&quot;] # join df_logs with df_joined_songs_artists df_log.join(df_joined_songs_artists, df_log.artist == df_joined_songs_artists.artist_name).select(songplay_cols).toPandas().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } timestamp userId song_id artist_id sessionId location userAgent level month year 0 2018-11-10 07:47:51.796 44 SOWQTQZ12A58A7B63E ARPFHN61187FB575F6 350 Waterloo-Cedar Falls, IA Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; r... paid 11 2018 1 2018-11-06 18:34:31.796 97 SOWQTQZ12A58A7B63E ARPFHN61187FB575F6 293 Lansing-East Lansing, MI \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5... paid 11 2018 2 2018-11-06 16:04:44.796 2 SOWQTQZ12A58A7B63E ARPFHN61187FB575F6 126 Plymouth, IN \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK... free 11 2018 3 2018-11-28 23:22:57.796 24 SOWQTQZ12A58A7B63E ARPFHN61187FB575F6 984 Lake Havasu City-Kingman, AZ \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK... paid 11 2018 4 2018-11-14 13:11:26.796 34 SOWQTQZ12A58A7B63E ARPFHN61187FB575F6 495 Milwaukee-Waukesha-West Allis, WI Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; r... free 11 2018 songplay_table_df = df_log.join(df_joined_songs_artists, df_log.artist == df_joined_songs_artists.artist_name).select(songplay_cols) songplay_table_df = songplay_table_df.withColumn(&quot;songplay_id&quot;, F.monotonically_increasing_id()) songplay_table_df.toPandas().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } timestamp userId song_id artist_id sessionId location userAgent level month year songplay_id 0 2018-11-10 07:47:51.796 44 SOWQTQZ12A58A7B63E ARPFHN61187FB575F6 350 Waterloo-Cedar Falls, IA Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; r... paid 11 2018 0 1 2018-11-06 18:34:31.796 97 SOWQTQZ12A58A7B63E ARPFHN61187FB575F6 293 Lansing-East Lansing, MI \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5... paid 11 2018 1 2 2018-11-06 16:04:44.796 2 SOWQTQZ12A58A7B63E ARPFHN61187FB575F6 126 Plymouth, IN \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK... free 11 2018 2 3 2018-11-28 23:22:57.796 24 SOWQTQZ12A58A7B63E ARPFHN61187FB575F6 984 Lake Havasu City-Kingman, AZ \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK... paid 11 2018 3 4 2018-11-14 13:11:26.796 34 SOWQTQZ12A58A7B63E ARPFHN61187FB575F6 495 Milwaukee-Waukesha-West Allis, WI Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; r... free 11 2018 4 # write this to parquet file # write to parquet file partition by songplay_table_df.write.parquet('data/songplays_table', partitionBy=['year', 'month'], mode='Overwrite') from pyspark.sql import functions as F from glob import glob test_df = spark.read.json(glob(&quot;test/*.json&quot;)) test_df.select(['song_id', 'title', 'artist_id', 'year', 'duration']).toPandas() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } song_id title artist_id year duration 0 SOYMRWW12A6D4FAB14 The Moon And I (Ordinary Day Album Version) ARKFYS91187B98E58F 0 267.70240 1 SOHKNRJ12A6701D1F8 Drop of Rain AR10USD1187B99F3F1 0 189.57016 2 SOYMRWW12A6D4FAB14 The Moon And SUN ASKFYS91187B98E58F 0 269.70240 3 SOUDSGM12AC9618304 Insatiable (Instrumental Version) ARNTLGG11E2835DDB9 0 266.39628 test_df.select(['song_id', 'title', 'artist_id', 'year', 'duration']).groupBy('song_id').count().show() +------------------+-----+ | song_id|count| +------------------+-----+ |SOUDSGM12AC9618304| 1| |SOYMRWW12A6D4FAB14| 2| |SOHKNRJ12A6701D1F8| 1| +------------------+-----+ test_df.select(F.col('song_id'), 'title') \\ .groupBy('song_id') \\ .agg({'title': 'first'}) \\ .withColumnRenamed('first(title)', 'title1') \\ .show() +------------------+--------------------+ | song_id| title| +------------------+--------------------+ |SOUDSGM12AC9618304|Insatiable (Instr...| |SOYMRWW12A6D4FAB14|The Moon And I (O...| |SOHKNRJ12A6701D1F8| Drop of Rain| +------------------+--------------------+ t1 = test_df.select(F.col('song_id'), 'title') \\ .groupBy('song_id') \\ .agg({'title': 'first'}) \\ .withColumnRenamed('first(title)', 'title1') t2 = test_df.select(['song_id', 'title', 'artist_id', 'year', 'duration']) t1.join(t2, 'song_id').where(F.col(&quot;title1&quot;) == F.col(&quot;title&quot;)).select([&quot;song_id&quot;, &quot;title&quot;, &quot;artist_id&quot;, &quot;year&quot;, &quot;duration&quot;]).show() +------------------+--------------------+------------------+----+---------+ | song_id| title| artist_id|year| duration| +------------------+--------------------+------------------+----+---------+ |SOYMRWW12A6D4FAB14|The Moon And I (O...|ARKFYS91187B98E58F| 0| 267.7024| |SOHKNRJ12A6701D1F8| Drop of Rain|AR10USD1187B99F3F1| 0|189.57016| |SOUDSGM12AC9618304|Insatiable (Instr...|ARNTLGG11E2835DDB9| 0|266.39628| +------------------+--------------------+------------------+----+---------+ ","link":"https://blog.ferretninja.com/post/aws-emr-spark-and-data-lake/"},{"title":"Java JUC (java.util.concurrent) 7 - Semaphore vs Lock Example","content":"1114. Print in Order class Foo { private volatile boolean onePrinted; private volatile boolean twoPrinted; public Foo() { onePrinted = false; twoPrinted = false; } public synchronized void first(Runnable printFirst) throws InterruptedException { // printFirst.run() outputs &quot;first&quot;. Do not change or remove this line. printFirst.run(); onePrinted = true; notifyAll(); } public synchronized void second(Runnable printSecond) throws InterruptedException { while(!onePrinted) { wait(); } // printSecond.run() outputs &quot;second&quot;. Do not change or remove this line. printSecond.run(); twoPrinted = true; notifyAll(); } public synchronized void third(Runnable printThird) throws InterruptedException { while(!twoPrinted) { wait(); } // printThird.run() outputs &quot;third&quot;. Do not change or remove this line. printThird.run(); } } import java.util.concurrent.*; class Foo { Semaphore run2, run3; public Foo() { run2 = new Semaphore(0); run3 = new Semaphore(0); } public void first(Runnable printFirst) throws InterruptedException { printFirst.run(); run2.release(); } public void second(Runnable printSecond) throws InterruptedException { run2.acquire(); printSecond.run(); run3.release(); } public void third(Runnable printThird) throws InterruptedException { run3.acquire(); printThird.run(); } } 1188. Design Bounded Blocking Queue When await() is called, the lock associated with this Condition is atomically released and the current thread becomes disabled for thread scheduling purposes. The current thread is assumed to hold the lock associated with this Condition when this method is called. newCondition() await/signal/signalAll synchonized wait/notify/notifyAll The ReentrantLock is an exclusive lock so only one thread can acquire the lock. See #1. When a thread call signal or signalAll, it releases respectively one thread or all threads awaiting for the corresponding Condition such that the thread or those threads will be eligible to acquire the lock again. But for now the lock is still owned by the thread that called signal or signalAll until it releases explicitly the lock by calling lock.unlock(). Then the thread(s) that has/have been released will be able to try to acquire the lock again, the thread that could acquire the lock will be able to check the condition again (by condition this time I mean count == items.length or count == 0 in this example), if it is ok it will proceed otherwise it will await again and release the lock to make it available to another thread. class BoundedBlockingQueue { private Queue&lt;Integer&gt; queue; private Semaphore consumerSemaphore; private Semaphore producerSemaphore; private Semaphore mutex; public BoundedBlockingQueue(int capacity) { this.queue = new LinkedList&lt;&gt;(); this.producerSemaphore = new Semaphore(capacity); this.consumerSemaphore = new Semaphore(0); this.mutex = new Semaphore(1); } public void enqueue(int element) throws InterruptedException { this.producerSemaphore.acquire(); //use mutex with Linkedlist or use ConcurrentLinkedDeque&lt;Integer&gt; q without mutex this.mutex.acquire(); this.queue.offer(element); this.mutex.release(); this.consumerSemaphore.release(); } public int dequeue() throws InterruptedException { this.consumerSemaphore.acquire(); this.mutex.acquire(); Integer res = this.queue.poll(); this.mutex.release(); this.producerSemaphore.release(); return res == null ? -1 : res; } public int size() { return this.queue.size(); } } class BoundedBlockingQueue { private Queue&lt;Integer&gt; queue; private int capacity; public BoundedBlockingQueue(int capacity) { this.capacity = capacity; this.queue = new LinkedList&lt;&gt;(); } public void enqueue(int element) throws InterruptedException { //object level synchronized so either enqueu or dequeue can run synchronized(this) { while (this.queue.size() == this.capacity) { wait(); } this.queue.offer(element); notifyAll(); } } public int dequeue() throws InterruptedException { synchronized(this) { while (this.queue.isEmpty()) { wait(); } int res = this.queue.poll(); notifyAll(); return res; } } public int size() { return this.queue.size(); } } class BoundedBlockingQueue { private int[] data; private int capacity = 0, head = 0, tail = 0; private volatile int size = 0; private Lock lock = new ReentrantLock(); private Condition isFull = lock.newCondition(), isEmpty = lock.newCondition(); public BoundedBlockingQueue(int capacity) { this.data = new int[capacity]; this.capacity = capacity; } public void enqueue(int element) throws InterruptedException { try { lock.lock(); while (size == capacity) { isFull.await(); // only one thread will get lock after all threads wake up by signalAll(). } data[tail++ % capacity] = element; size++; isEmpty.signalAll(); } catch(InterruptedException e) { } finally { lock.unlock(); } } public int dequeue() throws InterruptedException { int res = 0; try { lock.lock(); while (size == 0) { isEmpty.await(); } isFull.signalAll(); res = data[head++ % capacity]; size--; } catch(InterruptedException e) { } finally { lock.unlock(); } return res; } public int size() { return size; } } In order for the two methods to run concurrently they would have to use different locks class A { private final Object lockA = new Object(); private final Object lockB = new Object(); public void methodA() { synchronized(lockA) { //method A } } public void methodB() { synchronized(lockB) { //method B } } } ","link":"https://blog.ferretninja.com/post/java-juc-javautilconcurrent-7-semaphore-vs-lock-example/"},{"title":"Java JUC (java.util.concurrent) 6 - Exception","content":"To stop a thread stop() // deprecated throws ThreadDeath exception and release lock interrupt() // isInterrupted() public class MyThread extends Thread { @Override public void run() { try { for (int i=0; i&lt;50000; i++){ if (this.isInterrupted()) { System.out.println(&quot; 已经是停止状态了！&quot;); throw new InterruptedException(); // or intead use return; } System.out.println(i); } System.out.println(&quot; 不抛出异常，我会被执行的哦！&quot;); } catch (Exception e) { // e.printStackTrace(); } } public static void main(String[] args) throws InterruptedException { MyThread myThread =new MyThread(); myThread.start(); Thread.sleep(100); myThread.interrupt(); } } suspend() and resume() // deprecated because occupy lock suspend can block println() public void println(String x) { synchronized (this) { print(x); newLine(); } } uncaughtException ThreadGroup group = new ThreadGroup(&quot;&quot;){ @Override public void uncaughtException(Thread t, Throwable e) { super.uncaughtException(t, e); // 一个线程出现异常，中断组内所有线程 this.interrupt(); } }; public class MyThread{ public static void main(String[] args) { ThreadGroup threadGroup = new ThreadGroup(&quot;ThreadGroup&quot;){ @Override public void uncaughtException(Thread t, Throwable e) { System.out.println(&quot; 线程组的异常处理 &quot;); super.uncaughtException(t, e); } }; Thread.setDefaultUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() { @Override public void uncaughtException(Thread t, Throwable e) { System.out.println(&quot; 线程类的异常处理 &quot;); } }); Thread thread = new Thread(threadGroup,&quot;Thread&quot;){ @Override public void run() { System.out.println(Thread.currentThread().getName()+&quot; 执行 &quot;); int i= 2/0; } }; // thread.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() { // @Override // public void uncaughtException(Thread t, Throwable e) { // System.out.println(&quot; 线程对象的异常处理 &quot;); // } // }); thread.start(); } } //Thread 执行 //线程组的异常处理 //线程类的异常处理 ","link":"https://blog.ferretninja.com/post/java-juc-javautilconcurrent-6-exception/"},{"title":"Java JUC (java.util.concurrent) 5 - ThreadPool and ThreadLocal","content":"Java use Thread Pool pattern to save resources in a multithreaded application, and also to contain the parallelism in certain predefined limits. It controls several re-used threads for executing these tasks. ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 10; i++) { final int index = i; executorService.execute(new Runnable() { @Override public void run() { log.info(&quot;task:{}&quot;, index); } }); } executorService.shutdown(); There are several pre-defined threadpool for various scenarios. ExecutorService executorService = Executors.newFixedThreadPool(3); ExecutorService executorService = Executors.newSingleThreadExecutor(); ScheduledExecutorService executorService = Executors.newScheduledThreadPool(1); newFixedThreadPool = ThreadPoolExecutor with default public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); } The TheadLocal construct allows us to store data that will be accessible only by a specific thread. private final static ThreadLocal&lt;Long&gt; requestHolder = new ThreadLocal&lt;&gt;(); ","link":"https://blog.ferretninja.com/post/java-juc-javautilconcurrent-5-threadpool-and-threadlocal/"},{"title":"Java JUC (java.util.concurrent) 4 - Lock","content":"Compare to synchronized keyword, Locks support various methods for finer grained control thus are more expressive than synchronized. Since synchronized keyword create lock implicitly, one can convert the synchronized keyword to ReentrantLock. private synchronized static void add() { count++; } private final static Lock lock = new ReentrantLock(); private static void add() { lock.lock(); try { count++; } finally { lock.unlock(); } } Therefore, it is possible for both synchronized and Lock to introduce the well-know deadlock problem like below. public class DeadLock implements Runnable { public int flag = 1; //静态对象是类的所有对象共享的 private static Object o1 = new Object(), o2 = new Object(); @Override public void run() { log.info(&quot;flag:{}&quot;, flag); if (flag == 1) { synchronized (o1) { try { Thread.sleep(500); } catch (Exception e) { e.printStackTrace(); } synchronized (o2) { log.info(&quot;1&quot;); } } } if (flag == 0) { synchronized (o2) { try { Thread.sleep(500); } catch (Exception e) { e.printStackTrace(); } synchronized (o1) { log.info(&quot;0&quot;); } } } } public static void main(String[] args) { DeadLock td1 = new DeadLock(); DeadLock td2 = new DeadLock(); td1.flag = 1; td2.flag = 0; //td1,td2都处于可执行状态，但JVM线程调度先执行哪个线程是不确定的。 //td2的run()可能在td1的run()之前运行 new Thread(td1).start(); new Thread(td2).start(); } } The synchronized keyword provides a simplified model while Lock offers more APIs such as ReentrantReadWriteLock and Condition. The interface ReadWriteLock specifies a pair of locks for read and write access. This can improve performance and throughput in cases where write-accesses are much less frequent. private final Map&lt;String, Data&gt; map = new TreeMap&lt;&gt;(); private final ReentrantReadWriteLock lock = new ReentrantReadWriteLock(); private final Lock readLock = lock.readLock(); private final Lock writeLock = lock.writeLock(); public Data get(String key) { readLock.lock(); try { return map.get(key); } finally { readLock.unlock(); } } public Data put(String key, Data value) { writeLock.lock(); try { return map.put(key, value); } finally { writeLock.unlock(); } } Condition.signalAll() is a very important API in consumer-producer model. public static void main(String[] args) { ReentrantLock reentrantLock = new ReentrantLock(); Condition condition = reentrantLock.newCondition(); new Thread(() -&gt; { try { reentrantLock.lock(); log.info(&quot;wait signal&quot;); // 1 condition.await(); } catch (InterruptedException e) { e.printStackTrace(); } log.info(&quot;get signal&quot;); // 4 reentrantLock.unlock(); }).start(); new Thread(() -&gt; { reentrantLock.lock(); log.info(&quot;get lock&quot;); // 2 try { Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } condition.signalAll(); log.info(&quot;send signal ~ &quot;); // 3 reentrantLock.unlock(); }).start(); } ","link":"https://blog.ferretninja.com/post/java-juc-javautilconcurrent-4-lock/"},{"title":"Java JUC (java.util.concurrent) 3 - Synchronized and Synchronized Container","content":"Synchronized keyword bounds monitor to an object and has made concurrency programing simple. It can be added to method or object to achieve object level synchronization. It can also modify static method or class for class level synchronization. // 修饰一个代码块 public void test1(int j) { synchronized (this) { for (int i = 0; i &lt; 10; i++) { log.info(&quot;test1 {} - {}&quot;, j, i); } } } // 修饰一个方法 public synchronized void test2(int j) { for (int i = 0; i &lt; 10; i++) { log.info(&quot;test2 {} - {}&quot;, j, i); } } public static void main(String[] args) { SynchronizedExample1 example1 = new SynchronizedExample1(); SynchronizedExample1 example2 = new SynchronizedExample1(); ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -&gt; { example1.test2(1); }); executorService.execute(() -&gt; { example2.test2(2); }); } // 修饰一个类 public static void test1(int j) { synchronized (SynchronizedExample2.class) { for (int i = 0; i &lt; 10; i++) { log.info(&quot;test1 {} - {}&quot;, j, i); } } } // 修饰一个静态方法 public static synchronized void test2(int j) { for (int i = 0; i &lt; 10; i++) { log.info(&quot;test2 {} - {}&quot;, j, i); } } public static void main(String[] args) { SynchronizedExample2 example1 = new SynchronizedExample2(); SynchronizedExample2 example2 = new SynchronizedExample2(); ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -&gt; { example1.test1(1); }); executorService.execute(() -&gt; { example2.test1(2); }); } Java provide several thread safe synchronized containers out of the box. private static List&lt;Integer&gt; list = Collections.synchronizedList(Lists.newArrayList()); private static Set&lt;Integer&gt; set = Collections.synchronizedSet(Sets.newHashSet()); private static Map&lt;Integer, Integer&gt; map = Collections.synchronizedMap(new HashMap&lt;&gt;()); private static Map&lt;Integer, Integer&gt; map = new Hashtable&lt;&gt;(); private static List&lt;Integer&gt; list = new Vector&lt;&gt;(); Java and Guava also provide several immutable containers out of the box. private static Map&lt;Integer, Integer&gt; map = Maps.newHashMap(); static { map.put(1, 2); map.put(3, 4); map.put(5, 6); map = Collections.unmodifiableMap(map); } import com.google.common.collect.ImmutableList; import com.google.common.collect.ImmutableMap; import com.google.common.collect.ImmutableSet; private final static ImmutableList&lt;Integer&gt; list = ImmutableList.of(1, 2, 3); private final static ImmutableSet set = ImmutableSet.copyOf(list); private final static ImmutableMap&lt;Integer, Integer&gt; map = ImmutableMap.of(1, 2, 3, 4); private final static ImmutableMap&lt;Integer, Integer&gt; map2 = ImmutableMap.&lt;Integer, Integer&gt;builder() .put(1, 2).put(3, 4).put(5, 6).build(); ","link":"https://blog.ferretninja.com/post/java-juc-javautilconcurrent-3-synchronized-and-synchronized-container/"},{"title":"Java JUC (java.util.concurrent) 1 - AQS：AbstractQueuedSynchronizer","content":"AbstractQueuedSynchronizer lays the foundation of java JUC programming. Concurrency classes are extensively using Sync class which extends from AQS as shown below. Some example code snippets of CountDownLatch, CyclicBarrier, Semaphore, ForkJoinTask, and FutureTask are explained here. CountDownLatch public static void main(String[] args) throws Exception { ExecutorService exec = Executors.newCachedThreadPool(); final CountDownLatch countDownLatch = new CountDownLatch(threadCount); for (int i = 0; i &lt; threadCount; i++) { final int threadNum = i; exec.execute(() -&gt; { try { test(threadNum); } catch (Exception e) { log.error(&quot;exception&quot;, e); } finally { countDownLatch.countDown(); } }); } countDownLatch.await(10, TimeUnit.MILLISECONDS); log.info(&quot;finish&quot;); exec.shutdown(); } private static void test(int threadNum) throws Exception { Thread.sleep(100); log.info(&quot;{}&quot;, threadNum); } CyclicBarrier private static CyclicBarrier barrier = new CyclicBarrier(5); public static void main(String[] args) throws Exception { ExecutorService executor = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 10; i++) { final int threadNum = i; Thread.sleep(1000); executor.execute(() -&gt; { try { race(threadNum); } catch (Exception e) { log.error(&quot;exception&quot;, e); } }); } executor.shutdown(); } private static void race(int threadNum) throws Exception { Thread.sleep(1000); log.info(&quot;{} is ready&quot;, threadNum); barrier.await(); log.info(&quot;{} continue&quot;, threadNum); } Semaphore ExecutorService exec = Executors.newCachedThreadPool(); final Semaphore semaphore = new Semaphore(3); for (int i = 0; i &lt; threadCount; i++) { final int threadNum = i; exec.execute(() -&gt; { try { if (semaphore.tryAcquire(5000, TimeUnit.MILLISECONDS)) { // 尝试获取一个许可 test(threadNum); semaphore.release(); // 释放一个许可 } } catch (Exception e) { log.error(&quot;exception&quot;, e); } }); } exec.shutdown(); } private static void test(int threadNum) throws Exception { log.info(&quot;{}&quot;, threadNum); Thread.sleep(1000); } ForkJoinTask public class ForkJoinTaskExample extends RecursiveTask&lt;Integer&gt; { public static final int threshold = 2; private int start; private int end; public ForkJoinTaskExample(int start, int end) { this.start = start; this.end = end; } @Override protected Integer compute() { int sum = 0; //如果任务足够小就计算任务 boolean canCompute = (end - start) &lt;= threshold; if (canCompute) { for (int i = start; i &lt;= end; i++) { sum += i; } } else { // 如果任务大于阈值，就分裂成两个子任务计算 int middle = (start + end) / 2; ForkJoinTaskExample leftTask = new ForkJoinTaskExample(start, middle); ForkJoinTaskExample rightTask = new ForkJoinTaskExample(middle + 1, end); // 执行子任务 leftTask.fork(); rightTask.fork(); // 等待任务执行结束合并其结果 int leftResult = leftTask.join(); int rightResult = rightTask.join(); // 合并子任务 sum = leftResult + rightResult; } return sum; } public static void main(String[] args) { ForkJoinPool forkjoinPool = new ForkJoinPool(); //生成一个计算任务，计算1+2+3+4 ForkJoinTaskExample task = new ForkJoinTaskExample(1, 100); //执行一个任务 Future&lt;Integer&gt; result = forkjoinPool.submit(task); try { log.info(&quot;result:{}&quot;, result.get()); } catch (Exception e) { log.error(&quot;exception&quot;, e); } } } Future static class MyCallable implements Callable&lt;String&gt; { @Override public String call() throws Exception { log.info(&quot;do something in callable&quot;); Thread.sleep(5000); return &quot;Done&quot;; } } public static void main(String[] args) throws Exception { ExecutorService executorService = Executors.newCachedThreadPool(); Future&lt;String&gt; future = executorService.submit(new MyCallable()); log.info(&quot;do something in main&quot;); Thread.sleep(1000); String result = future.get(); log.info(&quot;result：{}&quot;, result); } FutureTask public static void main(String[] args) throws Exception { FutureTask&lt;String&gt; futureTask = new FutureTask&lt;String&gt;(new Callable&lt;String&gt;() { @Override public String call() throws Exception { log.info(&quot;do something in callable&quot;); Thread.sleep(5000); return &quot;Done&quot;; } }); new Thread(futureTask).start(); log.info(&quot;do something in main&quot;); Thread.sleep(1000); String result = futureTask.get(); log.info(&quot;result：{}&quot;, result); } ","link":"https://blog.ferretninja.com/post/java-juc-javautilconcurrent-1-aqsabstractqueuedsynchronizer/"},{"title":"Perfect Singleton","content":"A singleton class is a class that can have only one object (an instance of the class) at a time. Eager initialization is the mostly straightforward approach. @ThreadSafe public class SingletonExample2 { // 私有构造函数 private SingletonExample2() {} // 单例对象 private static SingletonExample2 instance = new SingletonExample2(); // 静态的工厂方法 public static SingletonExample2 getInstance() { return instance; } } Lazy initialization by adding synchronized keyword to the getInstance method is quite inefficient. @ThreadSafe @NotRecommend public class SingletonExample3 { // 私有构造函数 private SingletonExample3() { } // 单例对象 private static SingletonExample3 instance = null; // 静态的工厂方法 public static synchronized SingletonExample3 getInstance() { if (instance == null) { instance = new SingletonExample3(); } return instance; } } However, we can move the initialization into a static block so instance is created with class loading. @ThreadSafe public class SingletonExample6 { // 私有构造函数 private SingletonExample6() { } // 单例对象 private static SingletonExample6 instance = null; static { instance = new SingletonExample6(); } // 静态的工厂方法 public static SingletonExample6 getInstance() { return instance; } public static void main(String[] args) { System.out.println(getInstance().hashCode()); System.out.println(getInstance().hashCode()); } } To make Lazy initialization more efficient, it requires both double check locking method and volatile keyword to avoid CPU instruction reorder. @ThreadSafe public class SingletonExample5 { // 私有构造函数 private SingletonExample5() { } // 1、memory = allocate() 分配对象的内存空间 // 2、ctorInstance() 初始化对象 // 3、instance = memory 设置instance指向刚分配的内存 // 单例对象 volatile + 双重检测机制 -&gt; 禁止指令重排 private volatile static SingletonExample5 instance = null; // 静态的工厂方法 public static SingletonExample5 getInstance() { if (instance == null) { // 双重检测机制 // B synchronized (SingletonExample5.class) { // 同步锁 if (instance == null) { instance = new SingletonExample5(); // A - 3 } } } return instance; } } Finally, we can use enumeration approach. It has serialization and thread-safety guaranteed by the enum implementation itself, which ensures internally that only the single instance is available, correcting the problems pointed out in the class-based implementation. @ThreadSafe @Recommend public class SingletonExample7 { // 私有构造函数 private SingletonExample7() { } public static SingletonExample7 getInstance() { return Singleton.INSTANCE.getInstance(); } private enum Singleton { INSTANCE; private SingletonExample7 singleton; // JVM保证这个方法绝对只调用一次 Singleton() { singleton = new SingletonExample7(); } public SingletonExample7 getInstance() { return singleton; } } } ","link":"https://blog.ferretninja.com/post/perfect-singleton/"},{"title":"SQL 5 - Advanced","content":"Auto increment primary key Clustered index is (primary key -&gt; first unique not-nullable index -&gt; rowID) auto increment can maintain the B+ tree (only append new node at the end) Master-slave Master-slave asynchronous copy Latency due to single thread in slave to consume relay log Master-slave semi-synchronous copy read-write seperation through program (set slave read-only to prevent write to slave) through middle-ware (MyCAT) Latency happends for big transaction big table ddl master concurrent dml master no primary key Sharding When to shard Big table backup from slave takes too long Big table DDL takes too long how to shard vertical sharding frequently update field vertical sharding text/blob field vertical sharding of fast growing fields through program through middle-ware (MyCAT) Bulk delete if innodb_file_per_table on, each table has seperate file and drop table will clean space To delete a table alter table t29 rename t29_bak_20191011; wait a month to find dependence on the table drop table t29_bak_20191011; To clean a table (don't use delete because each row binlog, truncate will drop and create) create table t29_bak_20191011 like t29; insert into t29_bak_20191011 select * from t29; truncate table t29; back up table and then drop table t29_bak_20191011; delete some record in a table without partition back up table make sure index exist delete from table_name where date&lt;'2017-01-01' limit 1000; alter table table_name engine=InnoDB; or optimize table student; to release space (delete record will not free space, future insert can reuse) delete some record in a table with partition alter table t29_log drop partition p2016; ","link":"https://blog.ferretninja.com/post/sql-5-advanced/"},{"title":"SQL 4 - Transaction","content":"MVCC Multi version concurrency control In RC and RR, the session can see the record before other session's updating because of MVCC and undo-log. Good transaction practice submit outside loop do query that uses large lock range later in transaction two transactions get resources in same order choose RC is phantom read is fine don't mix innodb and mysiam table (mysiam doesn't support transaction and rollback) Distributed transaction mysql phase 1 prepare phase 2 commit/rollback message queue ","link":"https://blog.ferretninja.com/post/sql-4-transaction/"},{"title":"SQL 3 - Lock","content":"FLUSH TABLES WITH READ LOCK; UNLOCK TABLES; lock all tables. all tables read-only. current session write fail lock tables t14 write; current session can read and write, other session read/write will block. lock tables t14 read; all session can read. current session write will fail. other session write will block. metadata locking avoid long transaction and avoid ddl during database busy hour InnoDB support transaction and row locking S read row lock select * from table where ... lock in share mode; X write row lock select * from table whre ... for update; InnoBD locks: record lock on index keys gap lock between index keys next-key lock : record lock + gap lock if select doesn't use index, innoDB will lock table. Transactions level: Read uncommited. dirty read read commited. phantom read repeatable read. mysql default. serializable. for RC level, if select without index, table lock if unique index, lock matched index and corresponding clustered index if non unique index, lock matched index and corresponding clustered index for RR level if non unique index, GAP lock will lock the gap around matched index. if no index, table lock and gap lock locks all gaps. if unique index, same as RC level. Dead Lock to fix dead lock 1 innodb_deadlock_detect on to return error 2 innodb_lock_wait_timeout to time out Scenario different session lock different rows (same or different table) in different order. RR level, different session lock rows and write the row locked by each other. ","link":"https://blog.ferretninja.com/post/sql-3-lock/"},{"title":"Java TCP / UDP / HTTP / BIO / NIO","content":" class QuoteService { Map&lt;String, String&gt; productInfo = new HashMap&lt;String,String&gt;(); public QuoteService() { productInfo.put(&quot;a&quot;, &quot;100&quot;); productInfo.put(&quot;b&quot;, &quot;200&quot;); } public String getQuote(String product) { return productInfo.get(product); } } class ServiceThread extends Thread { Socket sock; QuoteService quoteService = new QuoteService(); public ServiceThread(Socket sock) { this.sock = sock; } public void run() { try { InputStream in = sock.getInputStream(); OutputStream out = sock.getOutputStream(); System.out.println(&quot;Waiting for product information from the client.&quot;); byte request[] = new byte[100]; in.read(request); String product = new String(request).trim(); System.out.println(&quot;Received product name - &quot; + product ); String price = quoteService.getQuote(product); if (price == null) { price = &quot;Invalid product&quot;; } out.write(price.getBytes()); System.out.println(&quot;Response sent...&quot;); sock.close(); } catch(Exception e) {} } } public class Server { public static void main(String[] args) throws IOException { ServerSocket serSocket = new ServerSocket(9999); System.out.println(&quot;Started listening to 9999&quot;); while (true) { System.out.println(&quot;Waiting for client..&quot;); Socket sock = serSocket.accept(); // create a new thread to service client. System.out.println(&quot;Starting a thread which will service the client&quot;); new ServiceThread(sock).start(); } } } public class Client { public static void main(String[] args) throws UnknownHostException, IOException { System.out.println(&quot;Connecting to the server...&quot;); Socket sock = new Socket(&quot;127.0.0.1&quot;, 9999); System.out.println(&quot;Connected to the server..&quot;); System.out.println(&quot;Enter product name : &quot;); Scanner scan = new Scanner(System.in); String product = scan.nextLine(); InputStream in = sock.getInputStream(); OutputStream out = sock.getOutputStream(); System.out.println(&quot;Sending product information..&quot;); out.write(product.getBytes()); byte [] response = new byte[100]; in.read(response); String strResponse = new String(response).trim(); System.out.println(&quot;Obtained response is - &quot; + strResponse); sock.close(); } } public class Server { public static void main(String[] args) throws IOException { DatagramSocket dgSock = new DatagramSocket(8989); DatagramPacket packet = new DatagramPacket(new byte[1000], 1000); dgSock.receive(packet); System.out.println( new String(packet.getData()) ); System.out.println(&quot;Obtained from IP - &quot; + packet.getAddress() ); System.out.println(&quot;Obtained from Port - &quot; + packet.getPort() ); dgSock.close(); } } public class Client { public static void main(String[] args) throws IOException { DatagramSocket dgSock = new DatagramSocket(); String message = &quot;Hello from so and so...&quot;; byte [] data = message.getBytes(); DatagramPacket packet = new DatagramPacket(data, data.length, InetAddress.getLocalHost(), 8989); dgSock.send(packet); dgSock.close(); } } public class RequestHandler { ResourceLoader resourceLoader = new ResourceLoader(); public void handleRequest(Socket sock) { OutputStream out = null; try { out = sock.getOutputStream(); String request = HttpUtils.getRequest(sock); String uri = HttpUtils.getRequestUri(request); System.out.println(&quot;Received request for - &quot; + uri); InputStream in = resourceLoader.getResource(uri); if (in == null) { System.out.println(&quot;Sending resource not found &quot;); HttpResponseUtils.sendResourceNotFound(out); return; } System.out.println(&quot;Sending response &quot;); HttpResponseUtils.sendSuccessResponse(in, out); } catch (Exception e) { e.printStackTrace(); if (out != null) { try { System.out.println(&quot;Sending internal error &quot;); HttpResponseUtils.sendInternalError(out); } catch (IOException e1) { e1.printStackTrace(); } } } finally { try { sock.close(); } catch (Exception e) { } } } } public class ServiceRequestTask implements Runnable { Socket sock; RequestHandler requestHandler = new RequestHandler(); public ServiceRequestTask(Socket sock) { this.sock = sock; } @Override public void run() { requestHandler.handleRequest(sock); } } public class Server { public static void main(String[] args) throws Exception { ServerSocket serSock = new ServerSocket(8000); ExecutorService executor = Executors.newFixedThreadPool(5); while (true) { System.out.println(&quot;Waiting for client...&quot;); Socket sock = serSock.accept(); System.out.println(&quot;Task submitted&quot;); executor.submit(new ServiceRequestTask(sock)); } } } BIO problem: blocking ServerSocket.accept() blocking InputStream.read(), outputStream.write() cannot handle multiple Stream I/O in same thread NIO: use Channel to replace Stream use Selector to monitor Channel status process multiple channel I/O in one thread Channel: FileChannel ServerSocketChannel SocketChannel ","link":"https://blog.ferretninja.com/post/java-tcp-udp-http-bio-nio/"},{"title":"Alibaba Java Coding Guidelines","content":" Switch add comment to case with no break. switch(condition){ case ABC: statements; /*程序继续执行直到 DEF 分支*/ case DEF: statements; break; case XYZ: statements; break; default: statements; break; } collections to array List&lt;String&gt; list = new ArrayList&lt;String&gt;(2); list.add(&quot;guan&quot;); list.add(&quot;bao&quot;); String[] array = new String[list.size()]; array = list.toArray(array); RuntimeException validate data and don't catch runtime exception try catch in transaction roll back transaction in catch manually or use spring @Transactional annotation Log use SLF4J API facade pattern not log4j or logback. import org.slf4j.Logger; import org.slf4j.LoggerFactory; public class HelloWorld{ private static final Logger logger = LoggerFactory.getLogger(HelloWorld.class); public static void main(String[] args){ logger.info(&quot;please use SLF4J,rather than logback or log4j&quot;); } } log retention 15 days Unit test use assert not system.out test cases don't call each other (use mock) and order independent ","link":"https://blog.ferretninja.com/post/alibaba-java-coding-guidelines/"},{"title":"SQL 2 - Index","content":"B tree vs B+ tree Auto increment primary key can prevent breaking the B+ tree. Clustered index - primary key if exist If no primary key, first not-nullable unique index or rowid. Only one clustered index and one B+ tree for a table. Non-clustered index B+ tree leaf node only have index field and primary key Search query will find primary key first and then fetch the record, therefore more costly. Where to have index where clause select * from t9_1 where d = 90000; aggregate function select max(d) from t9_1; select count(*) from t9_1; because count(*) use non-clustered index (only index and primary key) which is faster than clustered index. order by use only non-clustered index select b,c from t9_1 where b=90000; use idx_b_c Join clause turn BNL to NLJ search Difference between unique index and non-unique index non-clustered non-unique index use change buffer to combine multiple insert. Unique index has to load data into memory to check uniqueness. Although non-unique index will continue after finding the first matching record, it is highly possible that all matching records are loaded into memory in same page. Therefore, search query for unique index and non-unique index are the same. Multi-column indexes select * from t11 where a=1 and b=1 and c=1; use idx_a_b_c select * from t11 where c=1 and b=1 and a=1; /* sql2 */ same as first query, order doesn't matter select * from t11 where a=2 and b in (1,2) and c=2; /* sql3 */ select * from t11 where a=1 and b=2 order by c; /* sql4 */ select * from t11 where a=1 order by b,c; /* sql5 */ use idx_a_b_c idx_a_b_c is equivalent to idx_a, idx_a_b, idx_a_b_c. select * from t11 where a=2 and b in (3,4) order by c; /* sql13 */ will only use idx_a_b select * from t11 where b=1 and c=1; /* sql34 */ cannot use idx_a_b_c Cardinality show index from t13; Cardinality is the estimate of distinct index values. Query will use index of larger cardinality. If returned rows number is large, query may use the primary key index to fetch all records. Use force index to force specified index. ","link":"https://blog.ferretninja.com/post/sql-2-index/"},{"title":"Flash sale website system design","content":"Multi-level rate limiter prevent malicious requests add verification code redirect page after 5 seconds delay block ip hardware load-balancing - 2nd layer LVS load-balancing - 4th layer nginx reverse proxy to multiple tomcat - 7th layer tomcat max thread / max connection (io intensive, increase threads, increase throughput cpu intensive, decrease threads, increase cpu efficiency) message queue server cpu/mem monitoring RateLimiter Multi-level cache html css javascript in object storage service / CDN ajax / axis requests cache database requests in memory Cache penetration: malicous requests for nonexisting information use value = &quot;&quot; to cache malicous requests Cache breakdow: hot cache expiration, many request at the same expired cache use a thread to monitor cache near expiration and refresh cache use longer time for hot cache Cache avalanche: many cache expire at the same time use redis cluster and use different cache expiration time use local guavacache plus remote redis cluster use message queue as request buffer Fault Tolerance and service degradation Spring Cloud Hystrix manual shutdown of less important services Database cluster master dml -&gt; binlog -&gt; slave relay log -&gt; slave dml mycat cluster (use HaProxy + Keepalived) to sharding database write/read Thread safety lock/synchronzed keyword AtomicInteger compare and swap RateLimiter Leaky bucket smooth burst p drain rate Token bucket permit burst but bands it in any T, rate &lt; b + t * p, b bucket size, p token rate long term &lt; p JVM Reference Strong reference GC when reference = null or method exit SoftReference GC when memory not enough before throw OOM WeakReference collect whenever GC system.gc() PhantomReference put object in a queue when GC for further analysis use SoftReference to make cache, collect to prevent OOM class BigValue { ... //模拟大容量对象 } public class ReferenceCache { Map&lt;String, SoftReference&lt;BigValue&gt;&gt; caches = new HashMap(); //根据id存储缓存对象（缓存对象被装饰在了软引用中） void setCache(String id, BigValue bigValue) { caches.put(id, new SoftReference&lt;BigValue&gt;(bigValue)); } //根据id获取缓存对象 BigValue getCache(String id) { //根据id，获取缓存对象的软引用 SoftReference&lt;BigValue&gt; softRef = caches.get(id); return softRef == null ? null : softRef.get(); } } Message Queue Idempotent order -&gt; payment use db primary key to prevent duplicate insert use order id or unique mapping of order id as payment id use set or redis to make a history table to check before insert implement CAS to prevent update Message Queue Scaling increase consumer and partition write to new temparory queue and consume at new queue prevent retry Protobuf + Netty package com.yanqun.protobuf ; option java_package = &quot;com.yanqun.protobuf&quot; ; option java_outer_classname = &quot;MyMessage&quot; ; message MyMessage { required string name = 1 ; optional int32 age = 2 ; } cookie and session If cookie not allowed, put token in response. distributed session nginx ip_hash same ip goes to same server tomcat cluster copy session among all servers redis cluster Redis distributed lock Deployment ","link":"https://blog.ferretninja.com/post/flash-sale-website-system-design/"},{"title":"Java IO stream","content":" ","link":"https://blog.ferretninja.com/post/java-io-stream/"},{"title":"SQL 1 - Optimization and Best Practice","content":"Best practice use &quot;=&quot; not &quot;&lt;&gt;&quot; on index use Limit 1 if only return 1 record use TINYINT before SMALLINT and INT break big DELETE, UPDATE or INSERT into small ones use UNION ALL instead of UNION because ALL allows duplicate don't use select * which will not use index add index to where, join, order by use limit to return page Locate slow SQL query Use slow query log mysql&gt; set global slow_query_log = on; Query OK, 0 rows affected (0.00 sec) mysql&gt; show global variables like &quot;datadir&quot;; +---------------+------------------------+ | Variable_name | Value | +---------------+------------------------+ | datadir | /data/mysql/data/3306/ | +---------------+------------------------+ 1 row in set (0.00 sec) mysql&gt; show global variables like &quot;slow_query_log_file&quot;; +---------------------+----------------+ | Variable_name | Value | +---------------------+----------------+ | slow_query_log_file | mysql-slow.log | +---------------------+----------------+ 1 row in set (0.00 sec) Query_time: Lock_time: waiting for acquiring lock Rows_sent: number of rows returned Rows_examined: [root@mysqltest ~]# tail -n5 /data/mysql/data/3306/mysql-slow.log Time: 2019-05-21T09:15:06.255554+08:00 User@Host: root[root] @ localhost [] Id: 8591152 Query_time: 10.000260 Lock_time: 0.000000 Rows_sent: 1 Rows_examined: 0 SET timestamp=1558401306; select sleep(10); Use explain to analyze SQL query mysql&gt; explain select * from t1 where b=100; Scenario where Index won't be used mysql&gt; explain select * from t1 where date(c) ='2019-05-21'; Using native data format will use index condition. mysql&gt; explain select * from t1 where c&gt;='2019-05-21 00:00:00' and c&lt;='2019-05-21 23:59:59'; Implicit conversion won't use index. tele_phone is Varchar(20) select user_name,tele_phone from user_info where tele_phone =11111111111; /* SQL 1 */ Like clause using heading % won't. But if only trailing %, it can use index. mysql&gt; explain select * from t1 where a like '%1111%'; Large range won't use index mysql&gt; explain select * from t1 where b&gt;=1 and b &lt;=2000; b -1 = 100 won't and b = 100 + 1 can. mysql&gt; explain select * from t1 where b-1 =1000; Batch insert submission Insert multiple rows at a time create the batch insert sql [root@mysqltest muke]# mysqldump -utest_user3 -p'userBcdQ19Ic' -h127.0.0.1 --set-gtid-purged=off --single-transaction --skip-add-locks muke t1 &gt;t1.sql t1.sql ...... DROP TABLE IF EXISTS `t1`; ...... CREATE TABLE `t1`...... ...... INSERT INTO `t1` VALUES (1,'1',1,'2019-05-24 15:44:10'),(2,'2',2,'2019-05-24 15:44:10'),(3,'3',3,'2019-05-24 15:44:10')...... ...... [root@mysqltest muke]# mysqldump -utest_user3 -p'userBcdQ19Ic' -h127.0.0.1 --set-gtid-purged=off --single-transaction --skip-add-locks --skip-extended-insert muke t1 &gt;t1_row.sql -skip-extended-insert will insert one record at a time ...... INSERT INTO `t1` VALUES (1,'1',1,'2019-05-24 15:44:10'); INSERT INTO `t1` VALUES (2,'2',2,'2019-05-24 15:44:10'); INSERT INTO `t1` VALUES (3,'3',3,'2019-05-24 15:44:10'); ...... Run sql script [root@mysqltest ~]# time mysql -utest_user3 -p'userBcdQ19Ic' -h127.0.0.1 muke &lt;t1.sql Set auto commit off Adjust innodb_flush_log_at_trx_commmit and sync_binlog = 0 Order by and Group by mySql order by use two kinds of sorting. 1) using index 2) using filesort If data size &lt; sort_buffer_size, sort in memory (number of tmp files 0), else in disk (number of tmp files &gt; 0). Filesort has 3 modes. &lt;sort_key, rowid&gt; only sort rowid and fetch records after max_length_for_sort_data &lt; length of all fields &lt;sort_key, additional_fields&gt; loads all fields and sort max_length_for_sort_data &gt; length of all fields &lt;sort_key, packed_additional_fields&gt; Add index to sort by field order by (a , b) use index (a, b) not (b, a) where a = 1000 order by b use index (a, b) select * from t1 order by a,b; won't use index because scan index and fetch all records costs more than filesort. select id,a,b from t1 order by a,b; explain select id,a,b from t1 where a&gt;9000 order by b; won't use index because in range a &gt; 9000, data is not sorted by b. it is sorted by (a, b) explain select id,a,b from t1 order by a asc,b desc; won't use index Group by can use &quot;order by null&quot; to disable sorting. SQL Paging select * from t1 limit 99000,2; use auto-increment continuous primary key select * from t1 where id &gt;99000 limit 2; select * from t1 order by a limit 99000,2; won't use index because scan index and fetch all records costs more than filesort. 2. return id when sort select * from t1 f inner join (select id from t1 order by a limit 99000,2)g on f.id = g.id; Rewrite select * from t1 limit 99000,2; to select * from t1 as a join (select id from t1 limit 99000, 2) as b on a.id = b.id; SQL join Index Nested-Loop Join if join on index key. Small table will be the driving table. T ~ 2*N(small table). Block Nested-Loop join if join on key without index. Load small table into join_buffer. Scan each row of big table and compare it to each row in join_buffer. Total number of scanned row is M(big table) + N(small table). T ~ M(big table)*N(small table). join on index key use small table as driving table select * from t2 straight_join t1 on t2.a = t1.a; temporary table Add index to field b in temp table CREATE TEMPORARY TABLE `t1_tmp` ( `id` int(11) NOT NULL AUTO_INCREMENT, `a` int(11) DEFAULT NULL, `b` int(11) DEFAULT NULL, `create_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '记录创建时间', `update_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '记录更新时间', PRIMARY KEY (`id`), KEY `idx_a` (`a`), KEY `idx_b` (b) ) ENGINE=InnoDB ; insert into t1_tmp select * from t1; select * from t1_tmp join t2 on t1_tmp.b= t2.b; Count (*) Count(a) will not count null but Count(*) will. Count(1) and Count(*) will be the same. MyISAM has meta information about total number of rows. InnoDB doesn't have the meta information because it allows concurrent transactions to have different row count. MySql will use smallest non-clustered index to calculate count(*). If there is no non-clustered index, it will use clustered-index. Because non-clustered index (secondary) only store primary key while clustered-index (primary key) stores entire row. show table status like 't1'; will give an estimate of rows. use redis select count(\\*) from t1; set t1_count 10002 insert into t1(a,b,c,d) values (10003,10003,10003,10003); INCR t1_count delete from t1 where id=10003; DECR t1_count get t1_count problem: different session can read inaccurate result. Add inno db counting table ","link":"https://blog.ferretninja.com/post/sql-1-optimization-and-best-practice/"},{"title":"Java JUC (java.util.concurrent) 2 - Atomic Variables and Concurrent Collection","content":"Atomic varaibles are important parts of java concurrency API. It allows multiple threads safely access the same variables. The java.util.concurrent.atomic classes heavily used compare-and-swap (CAS) technique. CAS happens at CPU instruction level and is much more light weighted than synchronizing through locks or synchronized key word. AtomicInteger, AtomicLong, LongAdder, AtomicBoolean, and AtomicReference are some of the most used Atomic classes. AtomicInteger // 请求总数 public static int clientTotal = 5000; // 同时并发执行的线程数 public static int threadTotal = 200; public static AtomicInteger count = new AtomicInteger(0); public static void main(String[] args) throws Exception { ExecutorService executorService = Executors.newCachedThreadPool(); final Semaphore semaphore = new Semaphore(threadTotal); final CountDownLatch countDownLatch = new CountDownLatch(clientTotal); for (int i = 0; i &lt; clientTotal ; i++) { executorService.execute(() -&gt; { try { semaphore.acquire(); add(); semaphore.release(); } catch (Exception e) { log.error(&quot;exception&quot;, e); } countDownLatch.countDown(); }); } countDownLatch.await(); executorService.shutdown(); log.info(&quot;count:{}&quot;, count.get()); } private static void add() { count.incrementAndGet(); // count.getAndIncrement(); } Another way to acheive correct counting result like above is synchronized keyword. // 请求总数 public static int clientTotal = 5000; // 同时并发执行的线程数 public static int threadTotal = 200; public static int count = 0; public static void main(String[] args) throws Exception { ExecutorService executorService = Executors.newCachedThreadPool(); final Semaphore semaphore = new Semaphore(threadTotal); final CountDownLatch countDownLatch = new CountDownLatch(clientTotal); for (int i = 0; i &lt; clientTotal ; i++) { executorService.execute(() -&gt; { try { semaphore.acquire(); add(); semaphore.release(); } catch (Exception e) { log.error(&quot;exception&quot;, e); } countDownLatch.countDown(); }); } countDownLatch.await(); executorService.shutdown(); log.info(&quot;count:{}&quot;, count); } private synchronized static void add() { count++; } However, volatile keyword as in public static volatile int count = 0; will not be able to ensure threadsafety to due CPU instruction reorder. ConcurrentHashMap, ConcurrentSkipListMap, ConcurrentSkipListSet, CopyOnWriteArrayList, CopyOnWriteArraySet are some typical concurrency collection classes in Java 8. CopyOnWriteArrayList is a thread-safe variant of ArrayList where operations which can change the ArrayList (add, update, set methods) creates a clone of the underlying array. CopyOnWriteArrayList is to be used in a Thread based environment where read operations are very frequent and update operations are rare. Iterator of CopyOnWriteArrayList will never throw ConcurrentModificationException. Any type of modification to CopyOnWriteArrayList will not reflect during iteration since the iterator was created. List modification methods like remove, set and add are not supported in the iteration. This method will throw UnsupportedOperationException. null can be added to the list. CopyOnWriteArrayList // 请求总数 public static int clientTotal = 5000; // 同时并发执行的线程数 public static int threadTotal = 200; private static List&lt;Integer&gt; list = new CopyOnWriteArrayList&lt;&gt;(); public static void main(String[] args) throws Exception { ExecutorService executorService = Executors.newCachedThreadPool(); final Semaphore semaphore = new Semaphore(threadTotal); final CountDownLatch countDownLatch = new CountDownLatch(clientTotal); for (int i = 0; i &lt; clientTotal; i++) { final int count = i; executorService.execute(() -&gt; { try { semaphore.acquire(); update(count); semaphore.release(); } catch (Exception e) { log.error(&quot;exception&quot;, e); } countDownLatch.countDown(); }); } countDownLatch.await(); executorService.shutdown(); log.info(&quot;size:{}&quot;, list.size()); } private static void update(int i) { list.add(i); } ","link":"https://blog.ferretninja.com/post/java-juc-javautilconcurrent-2-atomic-variables-and-concurrent-collection/"},{"title":"Java 8 Lambda expressions and Streams","content":"Lambda Expressions Example public class Java8Tester { public static void main(String args[]) { Java8Tester tester = new Java8Tester(); //with type declaration MathOperation addition = (int a, int b) -&gt; a + b; //with out type declaration MathOperation subtraction = (a, b) -&gt; a - b; //with return statement along with curly braces MathOperation multiplication = (int a, int b) -&gt; { return a * b; }; //without return statement and without curly braces MathOperation division = (int a, int b) -&gt; a / b; System.out.println(&quot;10 + 5 = &quot; + tester.operate(10, 5, addition)); System.out.println(&quot;10 - 5 = &quot; + tester.operate(10, 5, subtraction)); System.out.println(&quot;10 x 5 = &quot; + tester.operate(10, 5, multiplication)); System.out.println(&quot;10 / 5 = &quot; + tester.operate(10, 5, division)); //without parenthesis GreetingService greetService1 = message -&gt; System.out.println(&quot;Hello &quot; + message); //with parenthesis GreetingService greetService2 = (message) -&gt; System.out.println(&quot;Hello &quot; + message); greetService1.sayMessage(&quot;Mahesh&quot;); greetService2.sayMessage(&quot;Suresh&quot;); } interface MathOperation { int operation(int a, int b); } interface GreetingService { void sayMessage(String message); } private int operate(int a, int b, MathOperation mathOperation) { return mathOperation.operation(a, b); } } IntBinaryOperator Represents an operation upon two int-valued operands and producing an int-valued result. IntConsumer Represents an operation that accepts a single int-valued argument and returns no result. IntFunction Represents a function that accepts an int-valued argument and produces a result. IntPredicate Represents a predicate (boolean-valued function) of one int-valued argument. IntSupplier Represents a supplier of int-valued results. //int applyAsInt(int left, int right) method in java.util.function.IntBinaryOperator static int method(IntBinaryOperator op){ return op.applyAsInt(5, 10); } //or define own interface: public interface TwoArgIntOperator { public int op(int a, int b); } static int method(TwoArgIntOperator operator) { return operator.op(5, 10); } Streams import java.util.ArrayList; import java.util.Arrays; import java.util.IntSummaryStatistics; import java.util.List; import java.util.Random; import java.util.stream.Collectors; import java.util.Map; public class Java8Tester { public static void main(String args[]) { System.out.println(&quot;Using Java 7: &quot;); // Count empty strings List&lt;String&gt; strings = Arrays.asList(&quot;abc&quot;, &quot;&quot;, &quot;bc&quot;, &quot;efg&quot;, &quot;abcd&quot;,&quot;&quot;, &quot;jkl&quot;); System.out.println(&quot;List: &quot; +strings); long count = getCountEmptyStringUsingJava7(strings); System.out.println(&quot;Empty Strings: &quot; + count); count = getCountLength3UsingJava7(strings); System.out.println(&quot;Strings of length 3: &quot; + count); //Eliminate empty string List&lt;String&gt; filtered = deleteEmptyStringsUsingJava7(strings); System.out.println(&quot;Filtered List: &quot; + filtered); //Eliminate empty string and join using comma. String mergedString = getMergedStringUsingJava7(strings,&quot;, &quot;); System.out.println(&quot;Merged String: &quot; + mergedString); List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5); //get list of square of distinct numbers List&lt;Integer&gt; squaresList = getSquares(numbers); System.out.println(&quot;Squares List: &quot; + squaresList); List&lt;Integer&gt; integers = Arrays.asList(1,2,13,4,15,6,17,8,19); System.out.println(&quot;List: &quot; +integers); System.out.println(&quot;Highest number in List : &quot; + getMax(integers)); System.out.println(&quot;Lowest number in List : &quot; + getMin(integers)); System.out.println(&quot;Sum of all numbers : &quot; + getSum(integers)); System.out.println(&quot;Average of all numbers : &quot; + getAverage(integers)); System.out.println(&quot;Random Numbers: &quot;); //print ten random numbers Random random = new Random(); for(int i = 0; i &lt; 10; i++) { System.out.println(random.nextInt()); } System.out.println(&quot;Using Java 8: &quot;); System.out.println(&quot;List: &quot; +strings); count = strings.stream().filter(string-&gt;string.isEmpty()).count(); System.out.println(&quot;Empty Strings: &quot; + count); count = strings.stream().filter(string -&gt; string.length() == 3).count(); System.out.println(&quot;Strings of length 3: &quot; + count); filtered = strings.stream().filter(string -&gt;!string.isEmpty()).collect(Collectors.toList()); System.out.println(&quot;Filtered List: &quot; + filtered); mergedString = strings.stream().filter(string -&gt;!string.isEmpty()).collect(Collectors.joining(&quot;, &quot;)); System.out.println(&quot;Merged String: &quot; + mergedString); squaresList = numbers.stream().map( i -&gt;i*i).distinct().collect(Collectors.toList()); System.out.println(&quot;Squares List: &quot; + squaresList); System.out.println(&quot;List: &quot; +integers); IntSummaryStatistics stats = integers.stream().mapToInt((x) -&gt;x).summaryStatistics(); System.out.println(&quot;Highest number in List : &quot; + stats.getMax()); System.out.println(&quot;Lowest number in List : &quot; + stats.getMin()); System.out.println(&quot;Sum of all numbers : &quot; + stats.getSum()); System.out.println(&quot;Average of all numbers : &quot; + stats.getAverage()); System.out.println(&quot;Random Numbers: &quot;); random.ints().limit(10).sorted().forEach(System.out::println); //parallel processing count = strings.parallelStream().filter(string -&gt; string.isEmpty()).count(); System.out.println(&quot;Empty Strings: &quot; + count); } private static int getCountEmptyStringUsingJava7(List&lt;String&gt; strings) { int count = 0; for(String string: strings) { if(string.isEmpty()) { count++; } } return count; } private static int getCountLength3UsingJava7(List&lt;String&gt; strings) { int count = 0; for(String string: strings) { if(string.length() == 3) { count++; } } return count; } private static List&lt;String&gt; deleteEmptyStringsUsingJava7(List&lt;String&gt; strings) { List&lt;String&gt; filteredList = new ArrayList&lt;String&gt;(); for(String string: strings) { if(!string.isEmpty()) { filteredList.add(string); } } return filteredList; } private static String getMergedStringUsingJava7(List&lt;String&gt; strings, String separator) { StringBuilder stringBuilder = new StringBuilder(); for(String string: strings) { if(!string.isEmpty()) { stringBuilder.append(string); stringBuilder.append(separator); } } String mergedString = stringBuilder.toString(); return mergedString.substring(0, mergedString.length()-2); } private static List&lt;Integer&gt; getSquares(List&lt;Integer&gt; numbers) { List&lt;Integer&gt; squaresList = new ArrayList&lt;Integer&gt;(); for(Integer number: numbers) { Integer square = new Integer(number.intValue() * number.intValue()); if(!squaresList.contains(square)) { squaresList.add(square); } } return squaresList; } private static int getMax(List&lt;Integer&gt; numbers) { int max = numbers.get(0); for(int i = 1;i &lt; numbers.size();i++) { Integer number = numbers.get(i); if(number.intValue() &gt; max) { max = number.intValue(); } } return max; } private static int getMin(List&lt;Integer&gt; numbers) { int min = numbers.get(0); for(int i= 1;i &lt; numbers.size();i++) { Integer number = numbers.get(i); if(number.intValue() &lt; min) { min = number.intValue(); } } return min; } private static int getSum(List numbers) { int sum = (int)(numbers.get(0)); for(int i = 1;i &lt; numbers.size();i++) { sum += (int)numbers.get(i); } return sum; } private static int getAverage(List&lt;Integer&gt; numbers) { return getSum(numbers) / numbers.size(); } } Kids With the Greatest Number of Candies class Solution { public List&lt;Boolean&gt; kidsWithCandies(int[] candies, int extraCandies) { int m = Arrays.stream(candies).max().getAsInt(); List&lt;Boolean&gt; res = new ArrayList&lt;&gt;(); res = Arrays.stream(candies).mapToObj(x -&gt; (x + extraCandies &gt;= m)).collect(Collectors.toList()); return res; } } ","link":"https://blog.ferretninja.com/post/java-8-lambda-expressions-and-streams/"},{"title":"JVM performance tuning","content":"Thread status - new, runnable, blocked, waiting, timed-waiting Thread dumps When the CPU Usage is Abnormally High Extract the thread that has the highest CPU usage. After acquiring the thread dump, check the thread's action. Extract thread dumps several times every hour, and check the status change of the threads to determine the problem. When the Processing Performance is Abnormally Slow After acquiring thread dumps several times, find the list of threads with BLOCKED status. Acquire the list of threads with BLOCKED status after getting the thread dumps several times. Heap dumps All Objects All Classes Garbage collection roots Thread Stacks and Local Variables Collector Types Throughput Collectors Serial Parallel Low Pause Collectors Concurrent Mark Sweep G1 ","link":"https://blog.ferretninja.com/post/jvm-performance-tuning/"},{"title":"Examples of System Design","content":" byte 1 byte -128 to 127. short 2 bytes -32,768 to 32,767. int 4 bytes -2,147,483,648 to 2,147,483,647. long 8 bytes -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807. Requirement clarification (use cases, post, follow, search, push notification) API Estimation (scaling, partitioning, load balancing, caching, storage, network bandwidth) Data model (SQL schema) Designing TinyURL Encoding actual URL We can compute a unique hash (e.g., MD5 or SHA256, etc.) of the given URL. The hash can then be encoded for displaying. This encoding could be base36 ([a-z ,0-9]) or base62 ([A-Z, a-z, 0-9]) and if we add ‘-’ and ‘.’ we can use base64 encoding. A reasonable question would be, what should be the length of the short key? 6, 8 or 10 characters. Using base64 encoding, a 6 letter long key would result in 64^6 = ~68.7 billion possible strings Using base64 encoding, an 8 letter long key would result in 64^8 = ~281 trillion possible strings With 68.7B unique strings, let’s assume six letter keys would suffice for our system. If we use the MD5 algorithm as our hash function, it’ll produce a 128-bit hash value. After base64 encoding, we’ll get a string having more than 21 characters (since each base64 character encodes 6 bits of the hash value). Since we only have space for 8 characters per short key, how will we choose our key then? We can take the first 6 (or 8) letters for the key. This could result in key duplication though, upon which we can choose some other characters out of the encoding string or swap some characters. What are different issues with our solution? We have the following couple of problems with our encoding scheme: If multiple users enter the same URL, they can get the same shortened URL, which is not acceptable. What if parts of the URL are URL-encoded? e.g., http://www.educative.io/distributed.php?id=design, and http://www.educative.io/distributed.php%3Fid%3Ddesign are identical except for the URL encoding. Workaround for the issues: We can append an increasing sequence number to each input URL to make it unique, and then generate a hash of it. We don’t need to store this sequence number in the databases, though. Possible problems with this approach could be an ever-increasing sequence number. Can it overflow? Appending an increasing sequence number will also impact the performance of the service. Another solution could be to append user id (which should be unique) to the input URL. However, if the user has not signed in, we would have to ask the user to choose a uniqueness key. Even after this, if we have a conflict, we have to keep generating a key until we get a unique one. Designing Pastebin Design Instagram a. Partitioning based on UserID Let’s assume we shard based on the ‘UserID’ so that we can keep all photos of a user on the same shard. If one DB shard is 1TB, we will need four shards to store 3.7TB of data. Let’s assume for better performance and scalability we keep 10 shards. So we’ll find the shard number by UserID % 10 and then store the data there. To uniquely identify any photo in our system, we can append shard number with each PhotoID. How can we generate PhotoIDs? Each DB shard can have its own auto-increment sequence for PhotoIDs and since we will append ShardID with each PhotoID, it will make it unique throughout our system. What are the different issues with this partitioning scheme? How would we handle hot users? Several people follow such hot users and a lot of other people see any photo they upload. Some users will have a lot of photos compared to others, thus making a non-uniform distribution of storage. What if we cannot store all pictures of a user on one shard? If we distribute photos of a user onto multiple shards will it cause higher latencies? Storing all photos of a user on one shard can cause issues like unavailability of all of the user’s data if that shard is down or higher latency if it is serving high load etc. b. Partitioning based on PhotoID If we can generate unique PhotoIDs first and then find a shard number through “PhotoID % 10”, the above problems will have been solved. We would not need to append ShardID with PhotoID in this case as PhotoID will itself be unique throughout the system. How can we generate PhotoIDs? Here we cannot have an auto-incrementing sequence in each shard to define PhotoID because we need to know PhotoID first to find the shard where it will be stored. One solution could be that we dedicate a separate database instance to generate auto-incrementing IDs. If our PhotoID can fit into 64 bits, we can define a table containing only a 64 bit ID field. So whenever we would like to add a photo in our system, we can insert a new row in this table and take that ID to be our PhotoID of the new photo. Wouldn’t this key generating DB be a single point of failure? Yes, it would be. A workaround for that could be defining two such databases with one generating even numbered IDs and the other odd numbered. For the MySQL, the following script can define such sequences: KeyGeneratingServer1: auto-increment-increment = 2 auto-increment-offset = 1 KeyGeneratingServer2: auto-increment-increment = 2 auto-increment-offset = 2 We can put a load balancer in front of both of these databases to round robin between them and to deal with downtime. Both these servers could be out of sync with one generating more keys than the other, but this will not cause any issue in our system. We can extend this design by defining separate ID tables for Users, Photo-Comments, or other objects present in our system. Alternately, we can implement a ‘key’ generation scheme similar to what we have discussed in Designing a URL Shortening service like TinyURL. Design Dropbox The metadata Database should be storing information about following objects: Chunks Files User Devices Workspace (sync folders) Designing Messenger Designing Twitter Designing Youtube At a high-level we would need the following components: Processing Queue: Each uploaded video will be pushed to a processing queue to be de-queued later for encoding, thumbnail generation, and storage. Encoder: To encode each uploaded video into multiple formats. Thumbnails generator: To generate a few thumbnails for each video. Video and Thumbnail storage: To store video and thumbnail files in some distributed file storage. User Database: To store user’s information, e.g., name, email, address, etc. Video metadata storage: A metadata database to store all the information about videos like title, file path in the system, uploading user, total views, likes, dislikes, etc. It will also be used to store all the video comments. Design Rate Limiter fixed window sliding wondow bucket counter if we have an hourly rate limit we can keep a count for each minute and calculate the sum of all counters in the past hour when we receive a new request to calculate the throttling limit Design Typeahead Suggestion Should we have case insensitive trie? For simplicity and search use-case, let’s assume our data is case insensitive. How to find top suggestion? Now that we can find all the terms for a given prefix, how can we find the top 10 terms for the given prefix? One simple solution could be to store the count of searches that terminated at each node, e.g., if users have searched about ‘CAPTAIN’ 100 times and ‘CAPTION’ 500 times, we can store this number with the last character of the phrase. Now if the user types ‘CAP’ we know the top most searched word under the prefix ‘CAP’ is ‘CAPTION’. So, to find the top suggestions for a given prefix, we can traverse the sub-tree under it. Given a prefix, how much time will it take to traverse its sub-tree? Given the amount of data we need to index, we should expect a huge tree. Even traversing a sub-tree would take really long, e.g., the phrase ‘system design interview questions’ is 30 levels deep. Since we have very strict latency requirements we do need to improve the efficiency of our solution. Can we store top suggestions with each node? This can surely speed up our searches but will require a lot of extra storage. We can store top 10 suggestions at each node that we can return to the user. We have to bear the big increase in our storage capacity to achieve the required efficiency. We can optimize our storage by storing only references of the terminal nodes rather than storing the entire phrase. To find the suggested terms we need to traverse back using the parent reference from the terminal node. We will also need to store the frequency with each reference to keep track of top suggestions. How would we build this trie? We can efficiently build our trie bottom up. Each parent node will recursively call all the child nodes to calculate their top suggestions and their counts. Parent nodes will combine top suggestions from all of their children to determine their top suggestions. How to update the trie? Assuming five billion searches every day, which would give us approximately 60K queries per second. If we try to update our trie for every query it’ll be extremely resource intensive and this can hamper our read requests, too. One solution to handle this could be to update our trie offline after a certain interval. As the new queries come in we can log them and also track their frequencies. Either we can log every query or do sampling and log every 1000th query. For example, if we don’t want to show a term which is searched for less than 1000 times, it’s safe to log every 1000th searched term. We can have a Map-Reduce (MR) set-up to process all the logging data periodically say every hour. These MR jobs will calculate frequencies of all searched terms in the past hour. We can then update our trie with this new data. We can take the current snapshot of the trie and update it with all the new terms and their frequencies. We should do this offline as we don’t want our read queries to be blocked by update trie requests. We can have two options: We can make a copy of the trie on each server to update it offline. Once done we can switch to start using it and discard the old one. Another option is we can have a master-slave configuration for each trie server. We can update slave while the master is serving traffic. Once the update is complete, we can make the slave our new master. We can later update our old master, which can then start serving traffic, too. How can we update the frequencies of typeahead suggestions? Since we are storing frequencies of our typeahead suggestions with each node, we need to update them too! We can update only differences in frequencies rather than recounting all search terms from scratch. If we’re keeping count of all the terms searched in last 10 days, we’ll need to subtract the counts from the time period no longer included and add the counts for the new time period being included. We can add and subtract frequencies based on Exponential Moving Average (EMA) of each term. In EMA, we give more weight to the latest data. It’s also known as the exponentially weighted moving average. After inserting a new term in the trie, we’ll go to the terminal node of the phrase and increase its frequency. Since we’re storing the top 10 queries in each node, it is possible that this particular search term jumped into the top 10 queries of a few other nodes. So, we need to update the top 10 queries of those nodes then. We have to traverse back from the node to all the way up to the root. For every parent, we check if the current query is part of the top 10. If so, we update the corresponding frequency. If not, we check if the current query’s frequency is high enough to be a part of the top 10. If so, we insert this new term and remove the term with the lowest frequency. How can we remove a term from the trie? Let's say we have to remove a term from the trie because of some legal issue or hate or piracy etc. We can completely remove such terms from the trie when the regular update happens, meanwhile, we can add a filtering layer on each server which will remove any such term before sending them to users. What could be different ranking criteria for suggestions? In addition to a simple count, for terms ranking, we have to consider other factors too, e.g., freshness, user location, language, demographics, personal history etc. Typeahead Client We can perform the following optimizations on the client side to improve user’s experience: The client should only try hitting the server if the user has not pressed any key for 50ms. If the user is constantly typing, the client can cancel the in-progress requests. Initially, the client can wait until the user enters a couple of characters. Clients can pre-fetch some data from the server to save future requests. Clients can store the recent history of suggestions locally. Recent history has a very high rate of being reused. Establishing an early connection with the server turns out to be one of the most important factors. As soon as the user opens the search engine website, the client can open a connection with the server. So when a user types in the first character, the client doesn’t waste time in establishing the connection. The server can push some part of their cache to CDNs and Internet Service Providers (ISPs) for efficiency. Designing Twitter Search Designing a Web Crawler Design news feed “Pull” model or Fan-out-on-load: This method involves keeping all the recent feed data in memory so that users can pull it from the server whenever they need it. Clients can pull the feed data on a regular basis or manually whenever they need it. Possible problems with this approach are a) New data might not be shown to the users until they issue a pull request, b) It’s hard to find the right pull cadence, as most of the time pull requests will result in an empty response if there is no new data, causing waste of resources. “Push” model or Fan-out-on-write: For a push system, once a user has published a post, we can immediately push this post to all the followers. The advantage is that when fetching feed you don’t need to go through your friend’s list and get feeds for each of them. It significantly reduces read operations. To efficiently handle this, users have to maintain a Long Poll request with the server for receiving the updates. A possible problem with this approach is that when a user has millions of followers (a celebrity-user) the server has to push updates to a lot of people. Hybrid: An alternate method to handle feed data could be to use a hybrid approach, i.e., to do a combination of fan-out-on-write and fan-out-on-load. Specifically, we can stop pushing posts from users with a high number of followers (a celebrity user) and only push data for those users who have a few hundred (or thousand) followers. For celebrity users, we can let the followers pull the updates. Since the push operation can be extremely costly for users who have a lot of friends or followers, by disabling fanout for them, we can save a huge number of resources. Another alternate approach could be that, once a user publishes a post, we can limit the fanout to only her online friends. Also, to get benefits from both the approaches, a combination of ‘push to notify’ and ‘pull for serving’ end users is a great way to go. Purely a push or pull model is less versatile. Should we always notify users if there are new posts available for their newsfeed? It could be useful for users to get notified whenever new data is available. However, on mobile devices, where data usage is relatively expensive, it can consume unnecessary bandwidth. Hence, at least for mobile devices, we can choose not to push data, instead, let users “Pull to Refresh” to get new posts. Design yelp Design uber ","link":"https://blog.ferretninja.com/post/examples-of-system-design/"},{"title":"NoSQL vs SQL","content":" RDMBS Benefit SQL Joins Aggregation good for small data volume Secondary index model data independent of Queries Drawback only scale vertically schema not flexible Normalization - reduce redundency and increase correctness denormalization - increase performance for read heavy Cassendra table - group of partition Partition - collection of rows - unit of access PK - partition key (Sharding) + clustering columns (sorting within partition desc) Cassandra Collection: Set, List, Map Good for logging events IOT time series db heavy write Bad for ad - hoc queries joins Denormalization is a must for cassendra / model queries no joins / one query per table MongoDB Embedded and Referenced Relationships Manual References { &quot;_id&quot;:ObjectId(&quot;52ffc33cd85242f436000001&quot;), &quot;contact&quot;: &quot;987654321&quot;, &quot;dob&quot;: &quot;01-01-1991&quot;, &quot;name&quot;: &quot;Tom Benzamin&quot;, &quot;address_ids&quot;: [ ObjectId(&quot;52ffc4a5d85242602e000000&quot;), ObjectId(&quot;52ffc4a5d85242602e000001&quot;) ] } &gt;var result = db.users.findOne({&quot;name&quot;:&quot;Tom Benzamin&quot;},{&quot;address_ids&quot;:1}) &gt;var addresses = db.address.find({&quot;_id&quot;:{&quot;$in&quot;:result[&quot;address_ids&quot;]}}) DBRefs { &quot;_id&quot;:ObjectId(&quot;53402597d852426020000002&quot;), &quot;address&quot;: { &quot;$ref&quot;: &quot;address_home&quot;, &quot;$id&quot;: ObjectId(&quot;534009e4d852427820000002&quot;), &quot;$db&quot;: &quot;tutorialspoint&quot;}, &quot;contact&quot;: &quot;987654321&quot;, &quot;dob&quot;: &quot;01-01-1991&quot;, &quot;name&quot;: &quot;Tom Benzamin&quot; } Covered Queries &gt;db.users.ensureIndex({gender:1,user_name:1}) Covered Query (fetch the required data from indexed data which is very fast. not go looking into database documents.) &gt;db.users.find({gender:&quot;M&quot;},{user_name:1,_id:0} Not Covered Query (index does not include _id field) &gt;db.users.find({gender:&quot;M&quot;},{user_name:1}) Atomic Operations &gt;db.products.findAndModify({ query:{_id:2,product_available:{$gt:0}}, update:{ $inc:{product_available:-1}, $push:{product_bought_by:{customer:&quot;rob&quot;,date:&quot;9-Jan-2014&quot;}} } }) Indexing Array Fields and Sub-Document Fields An ObjectId is a 12-byte BSON type having the following structure − The first 4 bytes representing the seconds since the unix epoch The next 3 bytes are the machine identifier The next 2 bytes consists of process id The last 3 bytes are a random counter value Text search &gt;db.adminCommand({setParameter:true,textSearchEnabled:true}) &gt;db.posts.ensureIndex({post_text:&quot;text&quot;}) &gt;db.posts.find({$text:{$search:&quot;tutorialspoint&quot;}}) Auto-Increment Sequence &gt;function getNextSequenceValue(sequenceName){ var sequenceDocument = db.counters.findAndModify({ query:{_id: sequenceName }, update: {$inc:{sequence_value:1}}, new:true }); return sequenceDocument.sequence_value; } Mongodb diagnosis and optimization web service response time &lt; 200ms mongodb response time &lt; 100ms long response time proper index use explain() cacheSizeGB ram size use mongostat() connection fail maxIncomingConnections db.serverStatus().connections shows available connections ulimit -a -&gt; open files -&gt; max file descriptors AWS redshift RDS table design optimization Distribution style Even - The leader node distributes the rows across the slices in a round-robin fashion Auto - Amazon Redshift assigns an optimal distribution style based on the size of the table data Key - The leader node places matching values on the same node slice ALL - replicate table on all nodes Sorting Key define a colum as sort key CREATE TABLE part ( p_partkey integer not null sortkey distkey, p_name varchar(22) not null, p_mfgr varchar(6) not null, p_category varchar(7) not null, p_brand1 varchar(9) not null, p_color varchar(11) not null, p_type varchar(25) not null, p_size integer not null, p_container varchar(10) not null ); Distribution key and sort key significantly improve query time Neo4J // Friend-of-a-friend (user)-[:KNOWS]-(friend)-[:KNOWS]-(foaf) // Shortest path path = shortestPath( (user)-[:KNOWS*..5]-(other) ) // Collaborative filtering (user)-[:PURCHASED]-&gt;(product)&lt;-[:PURCHASED]-()-[:PURCHASED]-&gt;(otherProduct) // Tree navigation (root)&lt;-[:PARENT*]-(leaf:Category)-[:ITEM]-&gt;(data:Product) ","link":"https://blog.ferretninja.com/post/nosql-vs-sql/"},{"title":"Javascript 1 - function, object, prototype","content":"Function definition //函数声明式定义 function foo(num1,num2){ return num1 + num2; } //函数表达式定义 var foo = function(num1,num2){ return num1 + num2; }; //使用Function构造函数定义 var foo = new Function(&quot;num1&quot;,&quot;num2&quot;,&quot;return num1 + num2&quot;); //实际上创建一个Function实例并不一定要赋值给具体的指针，可以直接执行 (function(x,y){return x+y})(1,2); //之所以用圆括号把function(){}括起来是因为js解释器会将function解释为函数声明，而函数声明不能直接跟着(x,y)，我们需要将其转换为函数表达式。 //(1,2)表示要传递跟函数的参数。 IIFE An IIFE (Immediately Invoked Function Expression) is a JavaScript function that runs as soon as it is defined. (function () { statements })();. In jQuery, the fn property is just an alias to the prototype property. jQuery identifier (or $) is just a constructor function. function Test() { this.a = 'a'; } Test.prototype.b = 'b'; var test = new Test(); test.a; // &quot;a&quot;, own property test.b; // &quot;b&quot;, inherited property (function() { var foo = function(arg) { // core constructor // ensure to use the `new` operator if (!(this instanceof foo)) return new foo(arg); // store an argument for this example this.myArg = arg; //.. }; // create `fn` alias to `prototype` property foo.fn = foo.prototype = { init: function () {/*...*/} //... }; // expose the library window.foo = foo; })(); // Extension: foo.fn.myPlugin = function () { alert(this.myArg); return this; // return `this` for chainability }; foo(&quot;bar&quot;).myPlugin(); // alerts &quot;bar&quot; Before ES6, to avoid global method name conflicts, add method to a object. After ES6, use commonJS. // bad let find = () =&gt; {}; let remove = () =&gt; {}; // good $.fn.find = () =&gt; {}; $.fn.remove = () =&gt; {}; // good (function($) { let find = () =&gt; {}; let remove = () =&gt; {}; $.fn.find = find; $.fn.remove = remove; }(jQuery)) 函数作为构造函数进行调用，this指向new出的那个对象 color = 'red'; var o = {color: 'blue'}; function sayColor() { console.log(this.color); } sayColor(); //red sayColor.call(this); //red sayColor.call(o); //blue &lt;script&gt; var x = 0; function test(){ this.x = 1; } var obj = new test(); console.log(obj.x); //1（说明this指向obj） &lt;/script&gt; JavaScript has no overload var sum(){ return arguments[0] + arguments[1]; //通过arguments对象执行内部操作 } console.log(sum(1, 2)); //3 function add(num1, num2){ return num1 + num2; } function add(value){ return value + 100; } console.log(add(1, 2)); //101 instance.proto === constructor.prototype var 对象 = new 函数() 对象.__proto__ === 对象的构造函数.prototype constructor1.prototype = instance2 鉴于上述游戏规则生效,如果试图引用constructor1构造的实例instance1的某个属性p1: 1).首先会在instance1内部属性中找一遍; 2).接着会在instance1.proto(constructor1.prototype)中找一遍,而constructor1.prototype 实际上是instance2, 也就是说在instance2中寻找该属性p1; 3).如果instance2中还是没有,此时程序不会灰心,它会继续在instance2.proto(constructor2.prototype)中寻找...直至Object的原型对象 搜索轨迹: instance1--&gt; instance2 --&gt; constructor2.prototype…--&gt;Object.prototype function Father(){ this.property = true; } Father.prototype.getFatherValue = function(){ return this.property; } function Son(){ this.sonProperty = false; } //继承 Father Son.prototype = new Father();//Son.prototype被重写,导致Son.prototype.constructor也一同被重写 Son.prototype.getSonVaule = function(){ return this.sonProperty; } var instance = new Son(); alert(instance.getFatherValue());//true alert(instance instanceof Object);//true alert(instance instanceof Father);//true alert(instance instanceof Son);//true alert(Object.prototype.isPrototypeOf(instance));//true alert(Father.prototype.isPrototypeOf(instance));//true alert(Son.prototype.isPrototypeOf(instance));//true New 第一行，我们创建了一个空对象obj; 第二行，我们将这个空对象的__proto__成员指向了F函数对象prototype成员对象; 第三行，我们将F函数对象的this指针替换成obj，然后再调用F函数. var obj = {}; obj.__proto__ = F.prototype; F.call(obj); Another way: subClass.prototype = superClass.prototype;//直接指向超类型prototype function Person() { } var person = new Person(); console.log(person.__proto__ == Person.prototype) // true console.log(Person.prototype.constructor == Person) // true // 顺便学习一个ES5的方法,可以获得对象的原型 console.log(Object.getPrototypeOf(person) === Person.prototype) // true console.log(person.constructor === Person); // true 当获取 person.constructor 时，其实 person 中并没有 constructor 属性,当不能读取到constructor 属性时，会从 person 的原型也就是 Person.prototype 中读取，正好原型中有该属性，所以：person.constructor === Person.prototype.constructor function vs object function is a object with prototype var o1 = {}; var o2 =new Object(); var o3 = new f1(); function f1(){}; var f2 = function(){}; var f3 = new Function('str','console.log(str)'); console.log(typeof Object); //function console.log(typeof Function); //function console.log(typeof f1); //function console.log(typeof f2); //function console.log(typeof f3); //function console.log(typeof o1); //object console.log(typeof o2); //object console.log(typeof o3); //object prototype function Person() {} Person.prototype = { name: 'Zaxlct', age: 28, job: 'Software Engineer', sayName: function() { console.log(this.name); } } var person1 = new Person(); person1.sayName(); // 'Zaxlct' var person2 = new Person(); person2.sayName(); // 'Zaxlct' console.log(person1.sayName == person2.sayName); //true 问：为什么 [1, 2, 3].map(parseInt) 返回 [1,NaN,NaN]? 答：parseInt 函数的第二个参数表示要解析的数字的基数。该值介于 2 ~ 36 之间。map will pass both value and index. ... for shallow copy var a = { name: 'ygy', age: 20 }; var b = { ...a }; use &amp;&amp; to replace if // old if (callback) callback(); // new callback &amp;&amp; callback(); // use &amp;&amp; to get value var obj = { info: { name: 'ygy' } } obj.info &amp;&amp; obj.info.name; // 'ygy' || to get default value var b = a || 10; // 如果a是空值，那么b就是10 This var a = 1; var obj1 = { a:2, fn:function(){ console.log(this.a); } } var fn1 = obj1.fn; var fn1bind = fn1.bind(obj1); fn1();//1 //window obj1.fn();//2 //object fn1bind();//2 fn1.call(obj1);//2 fn1.apply(obj1);//2 document.addEventListener('click', function(e){ console.log(this); //document setTimeout(function(){ console.log(this); //window }, 200); }, false); //constructor的this都指向实例 function Person(name,age){ this.name = name; this.age = age; this.sayAge = function(){ console.log(this.age); } } var dot = new Person('Dot',2); dot.sayAge();//2 Use closure to make variable private // 闭包版 const Student = function(age, sex) { let _age = age, _sex = sex; const setAge = (newAge) =&gt; { _age = 20; } const getAge = () =&gt; { return _age; } return { setAge, getAge } } const student = new Student(20, 'female'); console.log(student._age); // undefined ","link":"https://blog.ferretninja.com/post/javascript-1-function-object-prototype/"},{"title":"Data Metrics","content":"meaningful metrics -&gt; daily sessions per user time spent/session (the user is idle on the page) reactions, comments and shares in newfeed content. Reactions would include: likes, hearts, sad face, angry face etc. average number of interactions a user has per visit to the Newsfeed. click through rate for ads; this would help us understand whether ads are relevant Grow business -&gt; user retention or user acquisition user retention -&gt; increasing user engagement -&gt; metrics with time threshold ( Average likes per user per day) Improve product -&gt; feature demand that users already doing something despite a complicated user flow. Simplifying the flow will most likely improve your target metrics optimize a long term metric like retention rate or lifetime value -&gt; find a short term metric that can predict the long term one pick variables -&gt; pick a combination of user characteristics (age, sex, country, etc.) and behavioral ones (device, they came from ads/SEO/direct link, session time, etc.) Engagement on FB -&gt; proportion of users who take at least one action per day response rate on Quora -&gt; percentage of questions that get at least one response with at least 3 up-votes within a day Airbnb -&gt; if you want to go to a given place, you can do it uber new UI -&gt; AB test in two comparable markets (To identify required sample size, choose power, significance level, minimum difference between test and control, and std deviation) novelty effect -&gt; control for this by subsetting by drivers for which it is the first experience. Look at test results between new users in the control group vs new users in test group A/B test win but cost of change -&gt; Human labor costs (engineering time to make the change), opportunity-cost (not working on something else with a possibly higher expected value), Risk of bugs missing value in a varibale (Uber trips without rider review) -&gt; missing value is important information. predict missing value or use -1 as missing value e-commerce demand -&gt; going to a site and searching for &quot;jeans&quot;, ads click-through-rate (CTR) e-commerce supply -&gt; # conversions/# searches only considering people who used filters in their search. Or people whose session time is above a given value. site funnel -&gt; home page, search results, click on item, buy it predict Y (Instagram usage), how to find out whether X (here mobile Operative System - OS ) is a discriminant variable or not -&gt; 1. build a decision tree using user-related variables + OS to predict Instagram usage. 2. building two models: one including OS and one without. 3. generate simulated datasets where you adjust the distributions of all other variables. So that now you have the same age, country, etc, distribution for both iOS and Android subscription retention -&gt; percentage of users who don't unsubscribe within the first 12 months too long -&gt; proportion of people who unsubscribed or never used my product within the first week / three weeks user demographic vs behavioral characteristics -&gt; 1. Looking at a user browsing history gives information about what a user is interested in buying regardless of whether it is a gift or for herself. 2. Timing Browsing data tells the moment in which a certain user is thinking about buying a product. acquiring new users -&gt; new sign ups per day from users who send at least 1 message within the first 2 days retain current users -&gt; engagement -&gt; average messages per day per user new feature -&gt; find something that people are already doing, but in a complicated way requiring multiple steps. An example of this could be identifying that the last message of a conversation is about calling Uber, ordering food, or using any kind of other app. And a possible next step could be to integrate that functionality from within WhatsApp, kind of like Google Map can be called from inside WhatsApp. user lifetime value -&gt; pay for a click -&gt; revenue coming from that user within the first year -&gt; using short term data to predict -&gt; find features user location, user device, operative system, type of browser, source recommendation -&gt; shared connection, shared cluster (work friends, high school friends, university friends) predict fraud -&gt; Device ID, IP address, Ratings, Price, pictures, description, Browsing behavior that led to the seller creating the account. A/B test drawbacks -&gt; never be as similar markets, no full independence. Check one metric that's not supposed to be affected by your test. Make sure that during the test keeps behaving similarly for both markets customer service performance measurement -&gt; average user lifetime value (1 year) -&gt; user bought within 1 year after the ticket -&gt; Build a model to predict -&gt; response time and user feedback feature whether to add new feature -&gt; 1. good for site? engagement 2. demand. already doing it. 3. simply current flow. Two step authentication -&gt; ROC threshold -&gt; cost of false negatives (actual frauds happening) and value of true negatives (value of a legitimate user) -&gt; A/B testing, is the number of bad actors that two-step is blocking worth the number of good actors that the site is losing since it is harder to log-in. why a metric is down? -&gt; year over year metrics -&gt; numerator and denominator -&gt; if numberator down -&gt; new user are not liking as much as the usual ones or number of users is normal and number of likes suddenly down. -&gt; if new users are less engaged, find feature to predict &quot;up week&quot; new user and previous week old user -&gt; way more users from China this week. This might depend on a marketing campaign there that got a huge number of users, but these users are less engaged, as often when users come from sudden marketing campaigns. Or that all these new users come from very few different IP addresses. That would mean that all these users are probably fake accounts. 30 tests and 1 test (20 data segment countries and 1 segment country win) wins with p-value 0.04 -&gt; Bonferroni correction, simply dividing 0.05 by the number of tests and this becomes the new threshold for significance. -&gt; make the change only if test is better and p-value were less than 0.05/30. Test wins by 5%, Will that metric actually go up by ~5%, more, or less? -&gt; Control group numbers are likely inflated and it is likely that, if applied to all users, this change will lead to a larger gain than 5% vs old UI. cost of a false positive is way higher than false negative -&gt; recruiting process cost of a false positive is way lower than false negative -&gt; cancer detection how long I should run an A/B test? -&gt; 1. Significance level, usually 0.05. 2. Power, usually 0.8. 3. Expected standard deviation of the change in the metric. 4. Minimum effect size you are interested in detecting via your test. If the final number is less than 14 days, you still want to run the test for 14 days in order to reliably capture weekly patterns. We found a drop in pictures uploads. How to find out the reason? segment users by various features, such as browser, device, country, etc. Then you assume that you discovered that one segment dropped to zero. So you say it is likely a bug and, finally, explain where the bug could be. Isolate the impact of the algorithm and the UI change -&gt; Version 1 is the old version. Version 2 is the site with new Feature and machine learning model. Version 3 is the site with the People You May Know Feature, but suggestions are random or history-based model. detect fake information school -&gt; 1. email validation negatively affect legitimate users 2. user info from their profile + how they interacted with LinkedIn. (how many connection requests they sent, how they were distributed over time, acceptance rate, whether they visited other people profiles before sending the connection request.) build clusters. small dataset -&gt; 1. cross-validate 2. bootstrapping your original dataset. bagging predict job change -&gt; monthly -&gt; user profile data, data about when you took the snapshot, user behavior on the site, and some external data about job demand. response time of an inquiry at Airbnb -&gt; percentage of responses within 16 hrs is better than average response time considering only responses within 16 hrs because percentage consider all the population including people who never response. re-run the same A/B test -&gt; the underlying distribution of users has changed. early adopter vs new users identify clickbait -&gt; high than usual CTR + medium term (say 2 weeks) change in CTR revenue -&gt; 1. Increase CTR by better targeting. 2. Increase number of page views. 3. maximizing probability of conversion or working with the advertisers to improve the user flow after people click on an ad. H0 mu = 20 Ha mu &gt;= 20 A/B test power-&gt; p(reject H0 | H0 false) = 1 - p(not reject H0 | H0 false) = 1 - Type II error A/B test alpha 0.05 -&gt; Significance level. p-value lower than significance level reject null hypothesis A/B test p-value -&gt; take sample mean = 25, P(mu &gt;= sample mean| H0 true) &lt; 0.05 reject null hypothesis Uber: Monthly active platform consumer number of unique consumers who completed a ride or received an eats on platform in a given month Trips: number of completed ride or uber meal deliveries in a period Gross bookings: total dollar value Lyft: Active Rider Revenue per active rider Netflix: Paid membership subscription average avenue per user Pinterest: Mau authenticated user who visit at least once during 30 days ARPU total revenue divided by average number of mau in a period Facebook: DAU: A logged in user who visit at least one of the family product once on a given day MauL A logged in user who visit at least one of the family product once in the last 30 days ARPU Expedia: Room night growth gross booking: total retail value of transaction booked revenue per room night Spotify: Total monthly active user premium subscribers ad-supported MAUs ARPU Twitter: Monetizable DAU Logged in user that are able to show ads Snapchat: ARPU DAU registered user who opens application at least once during 24 hours ","link":"https://blog.ferretninja.com/post/data-metrics/"},{"title":"DAU and retention analysis","content":"Comparing test and control groups SELECT date_trunc('day', e.occurred_at), CASE WHEN flag = 'true' THEN 'treatment' ELSE 'control' END as flag, COUNT(e.event_name) FROM tutorial.yammer_events e WHERE e.event_name = 'login' GROUP BY 1,2 ORDER BY 1 DESC, 2 DESC Click through rates SELECT date_trunc('day', occurred_at) as day, 1.00 * COUNT (CASE WHEN action = 'email_clickthrough' THEN user_id ELSE NULL END) / COUNT (CASE WHEN action = 'email_open' THEN user_id ELSE NULL END) as CTR, COUNT (CASE WHEN action = 'email_clickthrough' THEN user_id ELSE NULL END) as clickthroughs, COUNT (CASE WHEN action = 'email_open' THEN user_id ELSE NULL END) as opens FROM tutorial.yammer_emails GROUP BY 1 --- WITH open as ( SELECT date_trunc('day', occurred_at) as day, COUNT(action) as opens FROM tutorial.yammer_emails WHERE action = 'email_open' GROUP BY 1 ), clickthrough as ( SELECT date_trunc('day', occurred_at) as day, COUNT(action) as clickthroughs FROM tutorial.yammer_emails WHERE action = 'email_clickthrough' GROUP BY 1 ) SELECT clickthrough.day, 1.00*clickthroughs/opens as CTR, clickthroughs, opens FROM clickthrough JOIN open ON clickthrough.day = open.day ORDER BY 1 DESC DAU, WAU, MAU, and ratios between them WITH dailies AS ( SELECT DATE_TRUNC('day', e.occurred_at) as date, COUNT(DISTINCT e.user_id) as dau FROM tutorial.yammer_events e WHERE e.event_name = 'login' GROUP BY 1 ) SELECT d, dau, (SELECT COUNT(DISTINCT e.user_id) as wau FROM tutorial.yammer_events e WHERE e.occurred_at::DATE BETWEEN dailies.date - 7 * Interval '1 day' AND dailies.date AND e.event_name = 'login' ) as wau_count, (SELECT COUNT(DISTINCT e.user_id) as mau FROM tutorial.yammer_events e WHERE e.occurred_at::DATE BETWEEN dailies.date - 30 * Interval '1 day' AND dailies.date AND e.event_name = 'login' ) as mau_count, 100.00 * dau/(SELECT COUNT(DISTINCT e.user_id) as mau FROM tutorial.yammer_events e WHERE e.occurred_at::DATE BETWEEN dailies.date - 30 * Interval '1 day' AND dailies.date AND e.event_name = 'login' ) as dau_mau FROM dailies UID, first_active_date, last_active_date, previous_active_date Retention with monthly_activity as ( select distinct date_trunc('month', created_at) as month, user_id from events ) select this_month.month, count(distinct user_id) from monthly_activity this_month join monthly_activity last_month on this_month.user_id = last_month.user_id and this_month.month = add_months(last_month.month,1) group by month Churn with monthly_activity as ( select distinct date_trunc('month', created_at) as month, user_id from events ) select last_month.month + add_months(last_month.month,1), count(distinct last_month.user_id) from monthly_activity last_month left join monthly_activity this_month on this_month.user_id = last_month.user_id and this_month.month = add_months(last_month.month,1) where this_month.user_id is null group by 1 Reactivated Users with monthly_activity as ( select distinct date_trunc('month', created_at) as month, user_id from events ), first_activity as ( select user_id, date(min(created_at)) as month from events group by 1 ) select this_month.month, count(distinct user_id) from monthly_activity this_month left join monthly_activity last_month on this_month.user_id = last_month.user_id and this_month.month = add_months(last_month.month,1) join first_activity on this_month.user_id = first_activity.user_id and first_activity.month != this_month.month where last_month.user_id is null group by 1 Percent Change with monthly_active_users as ( select date_trunc('month', created_at) as month, count (distinct user_id) as mau from events group by 1 ) select this_month.month, [(this_month.mau - last_month.mau)*1.0/last_month.mau:%] as pct_change from monthly_active_users this_month join monthly_active_users last_month on this_month.month = add_months(last_month.month,1) Sessionization SELECT * , extract(epoch from mytimestamp) - lag(extract(epoch from mytimestamp)) over (PARTITION BY user_id order by mytimestamp) as time_interval FROM toy_data_psql; SELECT * , CASE WHEN EXTRACT(EPOCH FROM mytimestamp) - LAG(EXTRACT(EPOCH FROM mytimestamp)) OVER (PARTITION BY user_id ORDER BY mytimestamp) &gt;= 30 * 60 THEN 1 ELSE 0 END as new_session FROM toy_data_psql; SELECT * , user_id || '_' || SUM(new_session) OVER (PARTITION BY user_id ORDER BY mytimestamp) AS session_id FROM ( SELECT * , CASE WHEN EXTRACT(EPOCH FROM mytimestamp) - LAG(EXTRACT(EPOCH FROM mytimestamp)) OVER (PARTITION BY user_id ORDER BY mytimestamp) &gt;= 30 * 60 THEN 1 ELSE 0 END as new_session FROM toy_data_psql ) s1 import dataiku import pandas as pd from datetime import timedelta # define treshold value T = timedelta(seconds=30*60) # load dataset toy_data = dataiku.Dataset(&quot;toy_data&quot;).get_dataframe() # add a column containing previous timestamp toy_data = pd.concat([toy_data, toy_data.groupby('user_id').transform(lambda x:x.shift(1))] ,axis=1) toy_data.columns = ['user_id','mytimestamp','prev_mytimestamp'] # create the new session column toy_data['new_session'] = ((toy_data['mytimestamp'] - toy_data['prev_mytimestamp'])&gt;=T).astype(int) # create the session_id toy_data['increment'] = toy_data.groupby(&quot;user_id&quot;)['new_session'].cumsum() toy_data['session_id'] = toy_data['user_id'].astype(str) + '_' + toy_data['increment'].astype(str) # to get the same result as with hive/postgresql toy_data = toy_data.sort(['user_id','mytimestamp']) ","link":"https://blog.ferretninja.com/post/dau-and-retention-analysis/"},{"title":"Rate limiting fundamentals","content":"Leaky bucket public abstract class RateLimiter { protected final int maxRequestPerSec; protected RateLimiter(int maxRequestPerSec) { this.maxRequestPerSec = maxRequestPerSec; } abstract boolean allow(); } public class LeakyBucket extends RateLimiter { private long nextAllowedTime; private final long REQUEST_INTERVAL_MILLIS; protected LeakyBucket(int maxRequestPerSec) { super(maxRequestPerSec); REQUEST_INTERVAL_MILLIS = 1000 / maxRequestPerSec; nextAllowedTime = System.currentTimeMillis(); } @Override boolean allow() { long curTime = System.currentTimeMillis(); synchronized (this) { if (curTime &gt;= nextAllowedTime) { nextAllowedTime = curTime + REQUEST_INTERVAL_MILLIS; return true; } return false; } } } Token Bucket Eager mode public class TokenBucket extends RateLimiter { private int tokens; public TokenBucket(int maxRequestsPerSec) { super(maxRequestsPerSec); this.tokens = maxRequestsPerSec; new Thread(() -&gt; { while (true) { try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } refillTokens(maxRequestsPerSec); } }).start(); } @Override public boolean allow() { synchronized (this) { if (tokens == 0) { return false; } tokens--; return true; } } private void refillTokens(int cnt) { synchronized (this) { tokens = Math.min(tokens + cnt, maxRequestPerSec); notifyAll(); } } } Lazy mode public class TokenBucketLazyRefill extends RateLimiter { private int tokens; private long lastRefillTime; public TokenBucketLazyRefill(int maxRequestPerSec) { super(maxRequestPerSec); this.tokens = maxRequestPerSec; this.lastRefillTime = System.currentTimeMillis(); } @Override public boolean allow() { synchronized (this) { refillTokens(); if (tokens == 0) { return false; } tokens--; return true; } } private void refillTokens() { long curTime = System.currentTimeMillis(); double secSinceLastRefill = (curTime - lastRefillTime) / 1000.0; int cnt = (int) (secSinceLastRefill * maxRequestPerSec); if (cnt &gt; 0) { tokens = Math.min(tokens + cnt, maxRequestPerSec); lastRefillTime = curTime; } } } Fixed Window Counter public class FixedWindowCounter extends RateLimiter { // TODO: Clean up stale entries private final ConcurrentMap&lt;Long, AtomicInteger&gt; windows = new ConcurrentHashMap&lt;&gt;(); protected FixedWindowCounter(int maxRequestPerSec) { super(maxRequestPerSec); } @Override boolean allow() { long windowKey = System.currentTimeMillis() / 1000 * 1000; windows.putIfAbsent(windowKey, new AtomicInteger(0)); return windows.get(windowKey).incrementAndGet() &lt;= maxRequestPerSec; } } Sliding Window Log public class SlidingWindowLog extends RateLimiter { private final Queue&lt;Long&gt; log = new LinkedList&lt;&gt;(); protected SlidingWindowLog(int maxRequestPerSec) { super(maxRequestPerSec); } @Override boolean allow() { long curTime = System.currentTimeMillis(); long boundary = curTime - 1000; synchronized (log) { while (!log.isEmpty() &amp;&amp; log.element() &lt;= boundary) { log.poll(); } log.add(curTime); return log.size() &lt;= maxRequestPerSec; } } } Sliding Window This is still not accurate becasue it assumes that the distribution of requests in previous window is even, which may not be true. But compares to fixed window counter, which only guarantees rate within each window, and sliding window log, which has huge memory footprint, sliding window is more practical. public class SlidingWindow extends RateLimiter { // TODO: Clean up stale entries private final ConcurrentMap&lt;Long, AtomicInteger&gt; windows = new ConcurrentHashMap&lt;&gt;(); protected SlidingWindow(int maxRequestPerSec) { super(maxRequestPerSec); } @Override boolean allow() { long curTime = System.currentTimeMillis(); long curWindowKey = curTime / 1000 * 1000; windows.putIfAbsent(curWindowKey, new AtomicInteger(0)); long preWindowKey = curWindowKey - 1000; AtomicInteger preCount = windows.get(preWindowKey); if (preCount == null) { return windows.get(curWindowKey).incrementAndGet() &lt;= maxRequestPerSec; } double preWeight = 1 - (curTime - curWindowKey) / 1000.0; long count = (long) (preCount.get() * preWeight + windows.get(curWindowKey).incrementAndGet()); return count &lt;= maxRequestPerSec; } } ","link":"https://blog.ferretninja.com/post/rate-limiting-fundamentals/"},{"title":"Build your own image hosting website using React/Spring Boot/Vsftpd","content":"Over the weekend, I have built an image hosting website. https://www.ferretninja.com/ An image hosting service helps to store images online and save local storage space. It provides a reliable backup and easy image sharing by URL that can be used by other websites. Moreover, the images will be available anytime so that one can share the link more easily with clients or friends. The details are published at https://medium.com/nerd-for-tech/build-your-own-image-hosting-website-using-react-spring-boot-vsftpd-on-aws-ec2-805fb971493d ","link":"https://blog.ferretninja.com/post/build-your-own-image-hosting-website-using-reactspring-bootvsftpd/"},{"title":"Write your own distributed job scheduling framework using ETCD and Spring Boot","content":"I published a tutorial on writing distributed job scheduler in Spring Boot with the help of ETCD on medium.com. ETCD, a distributed key-value store, saves the job list and provides a distributed lock for worker nodes. Spring Boot manages the connections to Redis Cluster cache and ETCD. The code for master and worker nodes are integrated into one repository and separately deployed using spring @profile annotation. Here is a link to the story, https://medium.com/@ywang412/write-your-own-distributed-job-scheduling-framework-using-etcd-and-spring-boot-83dbdb1a056b Today I found I earned two cents for this writing. I consider it is a significant step for me. The source code is hosted at github, https://github.com/ywang412/ants-job ","link":"https://blog.ferretninja.com/post/write-your-own-distributed-job-scheduling-framework-using-etcd-and-spring-boot/"},{"title":"Examples of OOD class diagrams","content":"Shopping Parking Library StackOverflow Movie Flight ATM public enum TransactionType { BALANCE_INQUIRY, DEPOSIT_CASH, DEPOSIT_CHECK, WITHDRAW, TRANSFER } public enum TransactionStatus { SUCCESS, FAILURE, BLOCKED, FULL, PARTIAL, NONE } public enum CustomerStatus { ACTIVE, BLOCKED, BANNED, COMPROMISED, ARCHIVED, CLOSED, UNKNOWN } public class Customer { private String name; private String email; private String phone; private Address address; private CustomerStatus status; private Card card; private Account account; public boolean makeTransaction(Transaction transaction); public Address getBillingAddress(); } public class Card { private String cardNumber; private String customerName; private Date cardExpiry; private int pin; public Address getBillingAddress(); } public class Account { private int accountNumber; private double totalBalance; private double availableBalance; public double getAvailableBalance(); } public class SavingAccount extends Account { private double withdrawLimit; } public class CheckingAccount extends Account { private String debitCardNumber; } public class ATM { private int atmID; private Address location; private CashDispenser cashDispenser; private Keypad keypad; private Screen screen; private Printer printer; private CheckDeposit checkDeposit; private CashDeposit cashDeposit; public boolean authenticateUser(); public boolean makeTransaction(Customer customer, Transaction transaction); } public abstract class Transaction { private int transactionId; private Date creationTime; private TransactionStatus status; public boolean makeTransation(); } public class BalanceInquiry extends Transaction { private int accountId; public double getAccountId(); } public abstract class Deposit extends Transaction { private double amount; public double getAmount(); } public class CheckDeposit extends Deposit { private String checkNumber; private String bankCode; public String getCheckNumber(); } public class CashDeposit extends Deposit { private double cashDepositLimit; } public class Withdraw extends Transaction { private double amount; public double getAmount(); } public class Transfer extends Transaction { private int destinationAccountNumber; public int getDestinationAccount(); } BlackJack Car rental Chess Stock Cric Info Hotel Linkedin Facebook Restaurants ","link":"https://blog.ferretninja.com/post/examples-of-ood-class-diagrams/"},{"title":"Hello Gridea","content":"👏 欢迎使用 Gridea ！ ✍️ Gridea 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ... Github Gridea 主页 示例网站 特性👇 📝 你可以使用最酷的 Markdown 语法，进行快速创作 🌉 你可以给文章配上精美的封面图和在文章任意位置插入图片 🏷️ 你可以对文章进行标签分组 📋 你可以自定义菜单，甚至可以创建外部链接菜单 💻 你可以在 Windows，MacOS 或 Linux 设备上使用此客户端 🌎 你可以使用 𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌 或 Coding Pages 向世界展示，未来将支持更多平台 💬 你可以进行简单的配置，接入 Gitalk 或 DisqusJS 评论系统 🇬🇧 你可以使用中文简体或英语 🌁 你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力 🖥 你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步 🌱 当然 Gridea 还很年轻，有很多不足，但请相信，它会不停向前 🏃 未来，它一定会成为你离不开的伙伴 尽情发挥你的才华吧！ 😘 Enjoy~ ","link":"https://blog.ferretninja.com/post/hello-gridea/"}]}